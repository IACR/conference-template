{
  "name": "Crypto 2021",
  "config": {
    "default_track_locations": [
      {
        "name": "Enter a location"
      }
    ],
    "default_talk_minutes": 25,
    "timezone": {
      "name": "UTC",
      "abbr": "UTC"
    },
    "unassigned_talks": [
      {
        "name": "Uncategorized",
        "talks": [],
        "id": "category-0"
      }
    ],
    "uniqueIDIndex": 167
  },
  "days": [
    {
      "date": "2021-08-16",
      "timeslots": [
        {
          "starttime": "16:00",
          "endtime": "16:10",
          "sessions": [
            {
              "session_title": "Opening remarks",
              "id": "session-1",
              "moderator": "General Chair, Program chairs"
            }
          ]
        },
        {
          "starttime": "16:10",
          "endtime": "16:50",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Signatures",
              "talks": [
                {
                  "paperId": "423",
                  "paperUrl": "https://www.theonion.com",
                  "title": "Threshold Schnorr with Stateless Deterministic Signing from Standard Assumptions",
                  "abstract": "Schnorr's signature scheme permits an elegant threshold signing protocol due to its linear signing equation. However each new signature consumes fresh randomness, which can be a major attack vector in practice. Sources of randomness in deployments are frequently either unreliable, or require state continuity, i.e. reliable fresh state resilient to rollbacks. State continuity is a notoriously difficult guarantee to achieve in practice, due to system crashes caused by software errors, malicious actors, or power supply interruptions (Parno et al., S&P '11). This is a non-issue for Schnorr variants such as EdDSA, which is specified to derive nonces deterministically as a function of the message and the secret key. However, it is challenging to translate these benefits to the threshold setting, specifically to construct a threshold Schnorr scheme where signing neither requires parties to consume fresh randomness nor update long-term secret state.\r\n\r\nIn this work, we construct a dishonest majority threshold Schnorr protocol that enables such stateless deterministic nonce derivation using standardized block ciphers. Our core technical ingredients are new tools for the zero-knowledge from garbled circuits (ZKGC) paradigm to aid in verifying correct nonce derivation:\r\n- A mechanism based on UC Commitments that allows a prover to commit once to a witness, and prove an unbounded number of statements online with only cheap symmetric key operations.\r\n- A garbling gadget to translate intermediate garbled circuit wire labels to arithmetic encodings.\r\n\r\nA proof per our scheme requires only a small constant number of exponentiations.",
                  "authors": [
                    "Francois Garillot",
                    "Yashvanth Kondi",
                    "Payman Mohassel",
                    "Valeria Nikolaenko"
                  ],
                  "affiliations": "; Northeastern University; Facebook; Novi/Facebook",
                  "category": "Uncategorized",
                  "id": "talk-132"
                },
                {
                  "paperId": "352",
                  "title": "Two-Round Trip Schnorr Multi-Signatures via Delinearized Witnesses",
                  "abstract": "We construct a two-round Schnorr-based signature scheme (DWMS)   by delinearizing\r\ntwo pre-witnesses supplied by each signer. DWMS is a secure signature scheme in the algebraic group model (AGM) and the random oracle model (ROM) under the assumption of the hardness of the one-more discrete logarithm problem and the 2-entwined sum problem that we introduce in this paper. Our new m-entwined sum problem tweaks the k-sum problem in a scalar field using the associated group.\r\nWe prove the hardness of our new problem in the AGM \r\nassuming the hardness of the discrete logarithm problem in the associated group.  We believe that our new problem simplifies the security proofs of multi-signature schemes that use the delinearization of witnesses.",
                  "authors": [
                    "Handan Kilinc Alper",
                    "Jeffrey Burdges"
                  ],
                  "affiliations": "Web3 Foundation",
                  "category": "Uncategorized",
                  "id": "talk-112"
                },
                {
                  "paperId": "100",
                  "title": "MuSig2: Simple Two-Round Schnorr Multi-Signatures",
                  "abstract": "Multi-signatures enable a group of signers to produce a single signature on a given message. Recently, Drijvers et al. (S&P’19) showed that all thus far proposed two-round multi-signature schemes in the DL setting (without pairings) are insecure under concurrent signing sessions. While Drijvers et al. propose a secure two-round scheme, this efficiency in terms of rounds comes with the price of having less compact signatures: the signatures are more than twice as large as Schnorr signatures, which are becoming popular in cryptographic systems due to their practicality (e.g., they will likely be adopted by Bitcoin). If one needs a multi-signature scheme that can be used as a drop-in replacement for Schnorr signatures, then one is forced to resort either to a three-round scheme or to sequential signing sessions, both of which are undesirable options in practice.\r\n\r\nIn this work, we propose MuSig2, a simple and highly practical two-round multi-signature scheme. Our scheme is the first that simultaneously i) is secure under concurrent signing sessions, ii) supports key aggregation, iii) outputs ordinary Schnorr signatures, iv) needs only two communicaion rounds, and v) has similar signer complexity as ordinary Schnorr signatures. Furthermore, our scheme is the first multi-signature scheme in the DL setting that supports preprocessing of all but one rounds, effectively enabling a non-interactive signing process without forgoing security under concurrent sessions. We prove the security of MuSig2 in the random oracle model, and the security of a more efficient variant in the combination of the random oracle and the algebraic group model. Both our proofs rely on a weaker variant of the OMDL assumption.",
                  "authors": [
                    "Jonas Nick",
                    "Tim Ruffing",
                    "Yannick Seurin"
                  ],
                  "affiliations": "Blockstream; ANSSI, France",
                  "category": "Uncategorized",
                  "id": "talk-51"
                },
                {
                  "paperId": "341",
                  "title": "Tighter Security for Schnorr Identification and Signatures: A High-Moment Forking Lemma for ${\\Sigma}$-Protocols",
                  "abstract": "The Schnorr identification and signature schemes have been amongst the most influential cryptographic protocols of the past three decades. Unfortunately, although the best-known attacks on these two schemes are via  discrete-logarithm computation, the known approaches for basing their security on the hardness of the discrete logarithm problem encounter the ``square-root barrier''. In particular, in any group of order $p$ where Shoup's generic hardness result for the discrete logarithm problem is believed to hold (and is thus used for setting concrete security parameters), the best-known $t$-time attacks on the Schnorr identification and signature schemes  have success probability $t^2/p$, whereas existing proofs of security only rule out attacks with success probabilities $(t^2/p)^{1/2}$ and $(q_{\\Hash} \\cdot t^2/p)^{1/2}$, respectively, where $q_{\\Hash}$ denotes the number of random-oracle queries issued by the attacker.\r\n\r\nWe establish tighter security guarantees for identification and signature schemes which result from $\\Sigma$-protocols with special soundness based on the hardness of their underlying relation, and in particular for Schnorr's schemes based on the hardness of the discrete logarithm problem. We circumvent the square-root barrier by introducing a high-moment generalization of the classic forking lemma, relying on the assumption that the underlying relation is ``$d$-moment hard'': The success probability of any algorithm in the task of producing a witness for a random instance is dominated by the $d$-th moment of the algorithm's running time. \r\n\r\nIn the concrete context of the discrete logarithm problem, already Shoup's original proof  shows that the discrete logarithm problem is $2$-moment hard in the generic-group model, and thus our assumption can be viewed as a highly-plausible strengthening of the discrete logarithm assumption in any group where no better-than-generic algorithms are currently known. Applying our  high-moment forking lemma in this context shows that, assuming the $2$-moment hardness of the discrete logarithm problem, any $t$-time attacker breaks the security of the Schnorr identification and signature schemes with probabilities at most $(t^2/p)^{2/3}$ and $(q_\\Hash \\cdot t^2/p)^{2/3}$, respectively.",
                  "authors": [
                    "Lior Rotem",
                    "Gil Segev"
                  ],
                  "affiliations": "The Hebrew University",
                  "category": "Uncategorized",
                  "id": "talk-108"
                }
              ],
              "id": "session-4"
            }
          ]
        },
        {
          "starttime": "16:50",
          "endtime": "17:00",
          "sessions": [
            {
              "id": "session-137",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "17:00",
          "endtime": "17:20",
          "sessions": [
            {
              "id": "session-138",
              "session_title": "Signatures (continued)",
              "talks": [
                {
                  "paperId": "190",
                  "title": "DualRing: Generic Construction of Ring Signatures with Efficient Instantiations",
                  "abstract": "We introduce a novel generic ring signature construction, called DualRing, which can be built from several canonical identification schemes (such as Schnorr identification). DualRing differs from the classical ring signatures by its formation of two rings: a ring of commitments and a ring of challenges. It has a structural difference from the common ring signature approaches based on accumulators or zero-knowledge proofs of the signer index. Comparatively, DualRing has a number of unique advantages.\r\n\r\nConsidering the DL-based setting by using Schnorr identification scheme, our DualRing structure allows the signature size to be compressed into logarithmic size via an argument of knowledge system such as Bulletproofs. We further improve on the Bulletproofs argument system to eliminate about half of the computation while maintaining the same proof size. We call this Sum Argument and it can be of independent interest. This DL-based construction, named DualRing-EC, using Schnorr identification with Sum Argument has the shortest ring signature size in the literature without using trusted setup.\r\n\r\nConsidering the lattice-based setting, we instantiate DualRing by a canonical identification based on M-LWE and M-SIS. In practice, we achieve the shortest lattice-based ring signature, named DualRing-LB, when the ring size is between 4 and 2000. DualRing-LB is also 5x faster in signing and verification than the fastest lattice-based scheme by Esgin et al. (CRYPTO'19).",
                  "authors": [
                    "Tsz Hon Yuen",
                    "Muhammed F. Esgin",
                    "Joseph K. Liu",
                    "Man Ho Au",
                    "Zhimin Ding"
                  ],
                  "affiliations": "The University of Hong Kong; Monash University and CSIRO's Data61; Monash University; Rice University",
                  "category": "Uncategorized",
                  "id": "talk-79"
                },
                {
                  "paperId": "376",
                  "title": "Compact Ring Signatures from Learning With Errors",
                  "abstract": "Ring signatures allow a user to sign a message on behalf of a ``ring'' of signers, while hiding the true identity of the signer. As the degree of anonymity guaranteed by a ring signature is directly proportional to the size of the ring, an important goal in cryptography is to study constructions that minimize the size of the signature as a function of the number of ring members.\r\n\r\nIn this work, we present the first compact ring signature scheme (i.e., where the size of the signature grows logarithmically with the size of the ring) from the (plain) learning with errors (LWE) problem. The construction is in the standard model and it does not rely on a trusted setup or on the random oracle heuristic. In contrast with the prior work of Backes\r\n\\etal~[EUROCRYPT'2019], our scheme does not rely on bilinear pairings, which allows us to show that the scheme is post-quantum secure assuming the quantum hardness of LWE.\r\n\r\nAt the heart of our scheme is a new construction of compact and statistically witness-indistinguishable ZAP arguments for NP $\\cap$ coNP, that we show to be sound based on the plain LWE assumption. Prior to our work, statistical ZAPs (for all of NP) were known to exist only assuming \\emph{sub-exponential} LWE. We believe that this scheme might find further applications in the future.",
                  "authors": [
                    "Rohit Chatterjee",
                    "Sanjam Garg",
                    "Mohammad Hajiabadi",
                    "Dakshita Khurana",
                    "Xiao Liang",
                    "Giulio Malavolta",
                    "Omkant Pandey",
                    "Sina Shiehian"
                  ],
                  "affiliations": "Stony Brook University; University of California, Berkeley and NTT Research; University of Waterloo; University of Illinois Urbana-Champaign; Max Planck Institute for Security and Privacy; University of California, Berkeley and Stony Brook University",
                  "category": "Uncategorized",
                  "id": "talk-118"
                }
              ]
            }
          ]
        },
        {
          "starttime": "17:20",
          "endtime": "17:50",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-5"
            }
          ]
        },
        {
          "starttime": "17:50",
          "endtime": "18:30",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Quantum Cryptography 1",
              "talks": [
                {
                  "paperId": "97",
                  "title": "A Black-Box Approach to Post-Quantum Zero-Knowledge in Constant Rounds",
                  "abstract": "In a recent seminal work, Bitansky and Shmueli (STOC '20) gave the first construction of a constant round zero-knowledge argument for NP secure against quantum attacks. However, their construction has several drawbacks compared to the classical counterparts. Specifically, their construction only achieves computational soundness, requires strong assumptions of quantum hardness of learning with errors (QLWE assumption) and the existence of quantum fully homomorphic encryption (QFHE), and relies on non-black-box simulation. In this paper, we resolve these issues at the cost of weakening the notion of zero-knowledge to what is called ϵ-zero-knowledge. Concretely, we construct the following protocols:\r\n\r\n- We construct a constant round interactive proof for NP that satisfies statistical soundness and black-box ϵ-zero-knowledge against quantum attacks assuming the existence of collapsing hash functions, which is a quantum counterpart of collision-resistant hash functions. Interestingly, this construction is just an adapted version of the classical protocol by Goldreich and Kahan (JoC '96) though the proof of ϵ-zero-knowledge property against quantum adversaries requires novel ideas.\r\n\r\n- We construct a constant round interactive argument for NP that satisfies computational soundness and black-box ϵ-zero-knowledge against quantum attacks only assuming the existence of post-quantum one-way functions.\r\n\r\nAt the heart of our results is a new quantum rewinding technique that enables a simulator to extract a committed message of a malicious verifier while simulating verifier's internal state in an appropriate sense.",
                  "authors": [
                    "Nai-Hui Chia",
                    "Kai-Min Chung",
                    "Takashi Yamakawa"
                  ],
                  "affiliations": "University of Maryland; Academia Sinica; NTT Secure Platform Laboratories",
                  "category": "Uncategorized",
                  "id": "talk-50"
                },
                {
                  "paperId": "67",
                  "title": "On the Concurrent Composition of Quantum Zero-Knowledge",
                  "abstract": "We study the notion of zero-knowledge secure against quantum polynomial-time verifiers (referred to as quantum zero-knowledge) in the concurrent composition setting. \r\n\r\nDespite being extensively studied in the classical setting, concurrent composition in the quantum setting has hardly been studied. \\par We initiate a formal study of concurrent quantum zero-knowledge. Our results are as follows:\r\n\r\n- Bounded Concurrent QZK for NP and QMA: Assuming post-quantum one-way functions, there exists a quantum zero-knowledge proof system for NP in the bounded concurrent setting. In this setting, we fix a priori the number of verifiers that can simultaneously interact with the prover. Under the same assumption, we also show that there exists a quantum zero-knowledge proof system for QMA in the bounded concurrency setting.\r\n\r\n- Quantum Proofs of Knowledge: Assuming quantum hardness of learning with errors (QLWE), there exists a bounded concurrent zero-knowledge proof system for NP satisfying quantum proof of knowledge property. \r\nOur extraction mechanism simultaneously allows for extraction probability to be negligibly close to acceptance probability (extractability) and also ensures that the prover's state after extraction is statistically close to the prover's state after interacting with the verifier (simulatability).\r\nEven in the standalone setting, the seminal work of [Unruh EUROCRYPT'12], and all its followups, satisfied a weaker version of extractability property and moreover, did not achieve simulatability. Our result yields a proof of {\\em quantum knowledge} system for QMA with better parameters than prior works.",
                  "authors": [
                    "Prabhanjan Ananth",
                    "Kai-Min Chung",
                    "Rolando L. La Placa"
                  ],
                  "affiliations": "UCSB; Academia Sinica, Taiwan; MIT",
                  "category": "Uncategorized",
                  "id": "talk-43"
                },
                {
                  "paperId": "431",
                  "title": "Multi-theorem Designated-Verifier NIZK for QMA",
                  "abstract": "Abstract. We present a designated-verifier non-interactive zero-knowledge argument\r\nsystem for QMA with multi-theorem security under the Learning with\r\nErrors Assumption. All previous such protocols for QMA are only single-theorem\r\nsecure. We also relax the setup assumption required in previous works. We prove\r\nsecurity in the malicious designated-verifier (MDV-NIZK) model (Quach, Rothblum,\r\nand Wichs, EUROCRYPT 2019), where the setup consists of a mutually\r\ntrusted random string and an untrusted verifier public key.\r\nOur main technical contribution is a general compiler that given a NIZK for NP\r\nand a quantum sigma protocol for QMA generates an MDV-NIZK protocol for\r\nQMA.",
                  "authors": [
                    "Omri Shmueli"
                  ],
                  "affiliations": "Tel Aviv University",
                  "category": "Uncategorized",
                  "id": "talk-133"
                },
                {
                  "paperId": "122",
                  "title": "On the Round Complexity of Secure Quantum Computation",
                  "abstract": "We study the round complexity of maliciously-secure quantum computation in the two-party (2PQC) and multi-party (MPQC) settings. Prior to our work, no constant-round protocols for maliciously-secure 2PQC or MPQC were known. We achieve the following results.\r\n\r\n1. Two-Message 2PQC with One-Sided Output. We construct a two-message protocol for two-party quantum computation in the common random string (CRS) model where one party receives output, assuming quantum-secure two-message oblivious transfer (OT). Such an OT protocol is known from the quantum hardness of Learning with Errors (QLWE). Combined with a reusable malicious designated-verifier non-interactive zero-knowledge (MDV-NIZK) argument for NP, the protocol gives an MDV-NIZK for QMA where the prover only requires one copy of the witness state.\r\n\r\n2. Constant-Round 2PQC and MPQC. We give the first constructions of maliciously-secure constant-round protocols for two-party and multi-party quantum computation in the CRS model where all parties receive output. \r\n\r\n- Assuming two-message OT in the CRS model, we obtain a three-message protocol for 2PQC and a five-round protocol for MPQC. The five-round protocol only requires three rounds of online communication, giving three-round MPQC in a quantum pre-processing model from two-message OT. \r\n\r\n- Assuming sub-exponentially secure QLWE, we obtain a three-round protocol for 2PQC and a four-round protocol for MPQC, both of which only require two online rounds of communication. This gives two-round MPQC in a quantum pre-processing model from sub-exponential LWE.\r\n\r\n3. Preliminary Investigation of Two-Round Protocols. We perform a preliminary investigation into barriers and possible approaches for two-round secure quantum computation without pre-processing. We provide evidence that protocols admitting a natural class of simulators do not exist, and also give a proof-of-concept protocol for two-round MPQC in the common reference string model from virtual black-box (VBB) obfuscation of quantum circuits.",
                  "authors": [
                    "James Bartusek",
                    "Andrea Coladangelo",
                    "Dakshita Khurana",
                    "Fermi Ma"
                  ],
                  "affiliations": "UC Berkeley; University of Illinois Urbana Champaign; Princeton University and NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-57"
                }
              ],
              "id": "session-6"
            }
          ],
          "twosessions": false
        },
        {
          "starttime": "18:30",
          "endtime": "18:40",
          "sessions": [
            {
              "id": "session-135",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "18:40",
          "endtime": "19:10",
          "sessions": [
            {
              "id": "session-136",
              "session_title": "Quantum Cryptography 1 (continued)",
              "talks": [
                {
                  "paperId": "223",
                  "title": "Round Efficient Secure Multiparty Quantum Computation with Identifiable Abort",
                  "abstract": "A recent result by Dulek et al. (EUROCRYPT 2020) showed a secure protocol for computing any quantum circuit even without the presence of an honest majority. Their protocol, however, is susceptible to a ``denial of service'' attack and allows even a single corrupted party to force an abort. We propose the first quantum protocol that admits security-with-identifiable-abort, which allows the honest parties to agree on the identity of a corrupted party in case of an abort.\r\n\r\nAdditionally, our protocol is the first to have the property that the number of rounds where quantum communication is required is independent of the circuit complexity. Furthermore, if there exists a post-quantum secure classical protocol whose round complexity is independent of the circuit complexity, then our protocol has this property as well. Our protocol is secure under the assumption that classical quantum-resistant fully homomorphic encryption schemes with decryption circuit of logarithmic depth exist.",
                  "authors": [
                    "Bar Alon",
                    "Hao Chung",
                    "Kai-Min Chung",
                    "Mi-Ying Huang",
                    "Yi Lee",
                    "Yu-Ching Shen"
                  ],
                  "affiliations": "Ariel University; Carnegie Mellon University; Academia Sinica",
                  "category": "Uncategorized",
                  "id": "talk-88"
                },
                {
                  "paperId": "156",
                  "title": "One-Way Functions Imply Secure Computation in a Quantum World",
                  "abstract": "We prove that quantum-hard one-way functions imply simulation-secure quantum oblivious transfer (QOT), which is known to suffice for secure computation of arbitrary quantum functionalities. Furthermore, our construction only makes black-box use of the quantum-hard one-way function.\r\n\r\nOur primary technical contribution is a construction of extractable and equivocal quantum bit commitments based on the black-box use of quantum-hard one-way functions in the standard model. Instantiating the Crépeau-Kilian (FOCS 1988) framework with these commitments yields simulation-secure quantum oblivious transfer.",
                  "authors": [
                    "James Bartusek",
                    "Andrea Coladangelo",
                    "Dakshita Khurana",
                    "Fermi Ma"
                  ],
                  "affiliations": "UC Berkeley; UIUC; Princeton University and NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-67"
                },
                {
                  "paperId": "178",
                  "title": "Impossibility of Quantum Virtual Black-Box Obfuscation of Classical Circuits",
                  "abstract": "Virtual black-box obfuscation is a strong cryptographic primitive: it encrypts a circuit while maintaining its full input/output functionality. A remarkable result by Barak et al. (Crypto 2001) shows that a general obfuscator that obfuscates classical circuits into classical circuits cannot exist. A promising direction that circumvents this impossibility result is to obfuscate classical circuits into quantum states, which would potentially be better capable of hiding information about the obfuscated circuit. We show that, under the assumption that Learning With Errors (LWE) is hard for quantum computers, this quantum variant of virtual black-box obfuscation of classical circuits is generally impossible. On the way, we show that under the presence of dependent classical auxiliary input, even the small class of classical point functions cannot be quantum virtual black-box obfuscated.",
                  "authors": [
                    "Gorjan Alagic",
                    "Zvika Brakerski",
                    "Yfke Dulek",
                    "Christian Schaffner"
                  ],
                  "affiliations": "QuICS, University of Maryland & National Institute of Standards and Technology; Weizmann Institute of Science; QuSoft & Centrum Wiskunde en Informatica; QuSoft & University of Amsterdam",
                  "category": "Uncategorized",
                  "id": "talk-74"
                }
              ]
            }
          ]
        },
        {
          "starttime": "19:10",
          "endtime": "19:40",
          "sessions": [
            {
              "id": "session-139",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "19:40",
          "endtime": "20:30",
          "sessions": [
            {
              "id": "session-140",
              "session_title": "Multi-Party Computation 1 ",
              "talks": [
                {
                  "paperId": "257",
                  "title": "Game-Theoretic Fairness Meets Multi-Party Protocols: The Case of Leader Election",
                  "abstract": "Suppose that $n$ players \r\nwant to elect a random leader and they communicate by posting\r\nmessages to a common broadcast channel.\r\nThis problem is called leader election, and it is \r\nfundamental to the distributed systems and cryptography literature. \r\nRecently, it has attracted renewed interests\r\ndue to its promised applications in decentralized environments.\r\nIn a game theoretically fair leader election protocol, roughly speaking,\r\nwe want that even majority coalitions \r\ncannot increase its own chance of getting \r\nelected, nor hurt the chance of any honest individual.\r\nThe folklore tournament-tree\r\nprotocol, which completes in logarithmically many rounds, \r\ncan easily be shown to satisfy game theoretic security. To the best of our knowledge,  \r\nno sub-logarithmic round protocol was known in the setting that we consider.\r\n\r\nWe show that\r\nby adopting an appropriate notion of approximate game-theoretic fairness,\r\nand under standard cryptographic assumption, \r\nwe can achieve \r\n$(1-1/2^{\\Theta(r)})$-fairness in $r$ rounds for $\\Theta(\\log \\log n) \\leq r \\leq \\Theta(\\log n)$, \r\n where $n$ denotes the number of players. In particular, this means that we can approximately match the fairness of the tournament tree protocol using as few as $O(\\log \\log n)$ rounds.\r\nWe also prove a lower bound showing that \r\nlogarithmically many rounds is necessary if we restrict ourselves \r\nto ``perfect'' game-theoretic fairness  \r\nand protocols that are \r\n``very similar in structure'' to the tournament-tree protocol.\r\n\r\nAlthough leader election is a well-studied problem in other contexts in distributed\r\ncomputing, \r\nour work is the first exploration of the round complexity \r\nof {\\it game-theoretically\r\nfair} leader election in the presence of a possibly majority coalition.\r\nAs a by-product of our exploration, \r\nwe suggest a new, approximate game-theoretic fairness \r\nnotion, called ``approximate sequential fairness'', \r\nwhich provides a more desirable solution concept than some previously\r\nstudied approximate fairness notions.",
                  "authors": [
                    "Kai-Min Chung",
                    "T-H. Hubert Chan",
                    "Ting Wen",
                    "Elaine Shi"
                  ],
                  "affiliations": "Academia Sinica; HKU; CMU",
                  "category": "Uncategorized",
                  "id": "talk-93"
                },
                {
                  "paperId": "319",
                  "title": "Computational Hardness of Optimal Fair Computation: Beyond Minicrypt",
                  "abstract": "Secure multi-party computation allows mutually distrusting parties to compute securely over their private data. However, guaranteeing output delivery to honest parties when the adversarial parties may abort the protocol has been a challenging objective. As a representative task, this work considers two-party coin-tossing protocols with guaranteed output delivery, a.k.a., fair coin-tossing. \r\n\r\nIn the information-theoretic plain model, as in two-party zero-sum games, one of the parties can force an output with certainty. In the commitment-hybrid, any $r$-message coin-tossing protocol is ${1/\\sqrt r}$-unfair, i.e., the adversary can change the honest party's output distribution by $1/\\sqrt r$ in the statistical distance. Moran, Naor, and Segev (TCC--2009) constructed the first $1/r$-unfair protocol in the oblivious transfer-hybrid. No further security improvement is possible because Cleve (STOC--1986) proved that $1/r$-unfairness is unavoidable. Therefore, Moran, Naor, and Segev's coin-tossing protocol is optimal. However, is oblivious transfer necessary for optimal fair coin-tossing? \r\n\r\nMaji and Wang (CRYPTO--2020) proved that any coin-tossing protocol using one-way functions in a black-box manner is at least $1/\\sqrt r$-unfair. That is, optimal fair coin-tossing is impossible in Minicrypt. Our work focuses on tightly characterizing the hardness of computation assumption necessary and sufficient for optimal fair coin-tossing within Cryptomania, outside Minicrypt. Haitner, Makriyannia, Nissim, Omri, Shaltiel, and Silbak (FOCS--2018 and TCC--2018) proved that better than $1/\\sqrt r$-unfairness, for any constant $r$, implies the existence of a key-agreement protocol. \r\n\r\nWe prove that any coin-tossing protocol using public-key encryption (or, multi-round key agreement protocols) in a black-box manner must be $1/\\sqrt r$-unfair. Next, our work entirely characterizes the additional power of secure function evaluation functionalities for optimal fair coin-tossing. We augment the model with an idealized secure function evaluation of $f$, \\aka, the $f$-hybrid. If $f$ is complete, that is,  oblivious transfer is possible in the $f$-hybrid, then optimal fair coin-tossing is also possible in the $f$-hybrid. On the other hand, if $f$ is not complete, then a coin-tossing protocol using public-key encryption in a black-box manner in the $f$-hybrid is at least $1/\\sqrt r$-unfair.",
                  "authors": [
                    "Hemanta K. Maji",
                    "Mingyuan Wang"
                  ],
                  "affiliations": "Purdue University",
                  "category": "Uncategorized",
                  "id": "talk-103"
                },
                {
                  "paperId": "229",
                  "title": "You Only Speak Once: Secure MPC with Stateless Ephemeral Roles",
                  "abstract": "The inherent difficulty of maintaining stateful environments over long periods of time gave rise to the paradigm of serverless computing, where mostly-stateless components are deployed on demand to handle computation tasks, and are teared down once their task is complete. Serverless architecture could offer the added benefit of improved resistance to targeted denial-of-service attacks, by hiding from the attacker the physical machines involved in the protocol until after they complete their work. Realizing such protection, however, requires that the protocol only uses stateless parties, where each party sends only one message and never needs to speaks again. Perhaps the most famous example of this style of protocols is the Nakamoto consensus protocol used in Bitcoin: A peer can win the right to produce the next block by running a local lottery (mining), all while staying covert.   Once the right has been won, it is executed by sending a single message. After that, the physical entity never needs to send more messages.\r\n\r\nWe refer to this as the You-Only-Speak-Once (YOSO) property, and initiate the formal study of it within a new model that we call the YOSO model. Our model is centered around the notion of roles, which are stateless parties that can only send a single message. Crucially, our modelling separates the protocol design, that only uses roles, from the role-assignment mechanism, that assigns roles to actual physical entities. This separation enables studying these two aspects separately, and our YOSO model in this work only deals with the protocol-design aspect.\r\n\r\nWe describe several techniques for achieving YOSO MPC; both computational and information theoretic. Our protocols are synchronous and provide guaranteed output delivery (which is important for application domains such as blockchains), assuming honest majority of roles in every time step. We describe a practically efficient computationally-secure protocol, as well as a proof-of-concept information theoretically secure protocol.",
                  "authors": [
                    "Craig Gentry",
                    "Shai Halevi",
                    "Hugo Krawczyk",
                    "Bernardo Magri",
                    "Jesper Buus Nielsen",
                    "Tal Rabin",
                    "Sophia Yakoubov"
                  ],
                  "affiliations": "Algorand Foundation; Concordium Blockchain Research Center, Aarhus University; UPenn and Algorand Foundation; Aarhus University",
                  "category": "Uncategorized",
                  "id": "talk-90"
                },
                {
                  "paperId": "304",
                  "title": "Fluid MPC: Secure Multiparty Computation with Dynamic Participants",
                  "abstract": "Existing approaches to secure multiparty computation (MPC) require all participants to commit to the entire duration of the protocol. As interest in MPC continues to grow, it is inevitable that there will be a desire to use it to evaluate increasingly complex functionalities, resulting in computations spanning several hours or days. \r\n\r\nSuch scenarios call for a *dynamic* participation model for MPC where participants have the flexibility to go offline as needed and (re)join when they have available computational resources. Such a model would also democratize access to privacy-preserving computation by facilitating an ``MPC-as-a-service'' paradigm --- the deployment of MPC in volunteer-operated networks (such as blockchains, where dynamism is inherent) that perform computation on behalf of clients. \r\n\r\nIn this work, we initiate the study of *fluid MPC*, where parties can dynamically join and leave the computation. The minimum commitment required from each participant is referred to as *fluidity*, measured in the number of rounds of communication that it must stay online. Our contributions are  threefold:\r\n\r\n- We provide a formal treatment of fluid MPC, exploring various possible modeling choices. \r\n- We construct information-theoretic fluid MPC protocols in the honest-majority setting. Our protocols achieve *maximal fluidity*, meaning that a party can exit the computation after receiving and sending messages in one round.\r\n- We implement our protocol and test it in multiple network settings.",
                  "authors": [
                    "Arka Rai Choudhuri",
                    "Aarushi Goel",
                    "Matthew Green",
                    "Abhishek Jain",
                    "Gabriel Kaptchuk"
                  ],
                  "affiliations": "Johns Hopkins University; Boston University",
                  "category": "Uncategorized",
                  "id": "talk-100"
                },
                {
                  "paperId": "188",
                  "title": "Secure Computation from One-Way Noisy Communication, or: Anti-Correlation via Anti-Concentration",
                  "abstract": "Can a sender encode a pair of messages (m_0,m_1) jointly, and send their encoding over (say) a binary erasure channel, so that the receiver can decode exactly one of the two messages and the sender does not know which one?\r\n\r\nGarg et al. (Crypto 2015) showed that this is information-theoretically impossible.\r\nWe show how to circumvent this impossibility by assuming that the receiver is computationally bounded, settling for an inverse-polynomial security error (which is provably necessary),  and relying on ideal obfuscation.\r\nOur solution creates a ``computational anti-correlation'' between the events of receiving m_0 and receiving m_1 by exploiting the anti-concentration of the binomial distribution.\r\n\r\nThe ideal obfuscation primitive in our construction can either be directly realized using (stateless) tamper-proof hardware, yielding an unconditional result, or heuristically instantiated using existing indistinguishability obfuscation schemes. We put forward a new notion of obfuscation that suffices to securely instantiate our construction.\r\n\r\nAs a corollary, we get similar feasibility results for general secure computation of sender-receiver functionalities by leveraging the completeness of the above ``random oblivious transfer'' functionality.",
                  "authors": [
                    "Shweta Agrawal",
                    "Yuval Ishai",
                    "Eyal Kushilevitz",
                    "Varun Narayanan",
                    "Manoj Prabhakaran",
                    "Vinod M. Prabhakaran",
                    "Alon Rosen"
                  ],
                  "affiliations": "Indian Institute of Technology Madras, India; Technion, Israel; Tata Institute of Fundamental Research, Mumbai, India; Indian Institute of Technology Bombay, India; IDC Herzliya, Israel",
                  "category": "Uncategorized",
                  "id": "talk-78"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "20:30",
          "endtime": "21:30",
          "sessions": [
            {
              "id": "session-141",
              "session_title": "Social Break "
            }
          ]
        },
        {
          "starttime": "21:30",
          "endtime": "22:30",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Which e-voting problems do we need to solve? ",
              "id": "session-2",
              "location": {
                "name": "Invited Talk by Vanessa Teague"
              }
            }
          ]
        }
      ]
    },
    {
      "date": "2021-08-17",
      "timeslots": [
        {
          "starttime": "15:00",
          "endtime": "16:00",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Award Papers",
              "talks": [
                {
                  "paperId": "342",
                  "title": "On the Possibility of Basing Cryptography on $\\EXP \\neq \\BPP$",
                  "abstract": "Liu and Pass (FOCS'20) recently demonstrated an equivalence between the\r\nexistence of one-way\r\nfunctions and mild average-case hardness of the time-bounded\r\nKolmogorov complexity problem. In this work, we establish a similar\r\nequivalence but to a different form of time-bounded Kolmogorov\r\nComplexity---namely, Levin's notion of Kolmogorov Complexity---whose\r\nhardness is closely related to the problem of whether $\\EXP \\neq\r\n\\BPP$. In more detail, let $Kt(x)$ denote the Levin-Kolmogorov Complexity of the string $x$;\r\nthat is, $Kt(x) = \\min_{\\desc \\in \\bitset^*, t \\in \\N}\\{|\\desc| +\r\n\\lceil \\log t \\rceil: U(\\desc, 1^t) = x\\}$, where $U$ is a universal\r\nTuring machine, and let $\\mktp$ denote the language of pairs $(x,k)$ having\r\nthe property that $Kt(x) \\leq k$.\r\nWe demonstrate that:\r\n - $\\mktp$ is \\emph{two-sided error} mildly average-case hard (i.e., $\\mktp\r\n  \\notin \\HeurpBPP$) iff infinititely-often one-way\r\n  functions exist.\r\n- $\\mktp$ is \\emph{errorless} mildly average-case hard (i.e., $\\mktp\r\n    \\notin \\AvgpBPP$) iff $\\EXP \\neq \\BPP$.\r\nThus, the only ``gap'' towards getting (infinitely-often) one-way\r\nfunctions from the assumption that $\\EXP \\neq \\BPP$ is the\r\nseemingly ``minor'' technical gap\r\nbetween two-sided error and errorless average-case hardness of the\r\n$\\mktp$ problem.\r\n\r\nAs a corollary of this result, we additionally demonstrate that\r\nany reduction from errorless to two-sided error average-case\r\nhardness for $\\mktp$ implies (unconditionally) that $\\NP \\neq \\P$. \r\n\r\nWe finally consider other alternative notions of Kolmogorov\r\ncomplexity---including space-bounded Kolmogorov complexity and\r\nconditional Kolmogorov complexity---and show how average-case\r\nhardness of problems related to them characterize log-space\r\ncomputable one-way functions, or one-way functions in $\\NC^0$.",
                  "authors": [
                    "Yanyi Liu",
                    "Rafael Pass"
                  ],
                  "affiliations": "Cornell University; Cornell Tech",
                  "category": "Uncategorized",
                  "id": "talk-109"
                },
                {
                  "paperId": "83",
                  "title": "Linear Cryptanalysis of FF3-1 and FEA",
                  "abstract": "Improved attacks on generic small-domain Feistel ciphers with alternating round tweaks are obtained using linear cryptanalysis. This results in practical distinguishing and message-recovery attacks on the United States format-preserving encryption standard FF3-1 and the South-Korean standards FEA-1 and FEA-2. The data-complexity of the proposed attacks on FF3-1 and FEA-1 is $O(N^{r/2 - 1.5})$, where $N^2$ is the domain size and $r$ is the number of rounds. For example, FF3-1 with $N = 10^3$ can be distinguished from an ideal tweakable block cipher with advantage $\\ge 1/10$  using $2^{23}$ encryption queries. Recovering the left half of a message with similar advantage requires $2^{24}$ data. The analysis of FF3-1 serves as an interesting real-world application of (generalized) linear cryptanalysis over the group $\\mathbb{Z}/N\\mathbb{Z}$.",
                  "authors": [
                    "Tim Beyne"
                  ],
                  "affiliations": "imec-COSIC, ESAT, KU Leuven",
                  "category": "Uncategorized",
                  "id": "talk-47"
                },
                {
                  "paperId": "168",
                  "title": "Efficient Key Recovery for all HFE Signature Variants",
                  "abstract": "The HFE cryptosystem is one of the best known multivariate schemes. Especially in the area of digital signatures, the HFEv- variant offers short signatures and high performance. Recently, an instance of the HFEv- signature scheme called GeMSS was elected as one of the alternative candidates for signature schemes in the third round of the NIST Post Quantum Crypto (PQC) Standardization Project. In this paper, we propose a new key recovery attack on the HFEv- signature scheme. Our attack shows that both the Minus and the Vinegar modifi- cation do not enhance the security of the basic HFE scheme significantly. This shows that it is very difficult to build a secure and efficient signature scheme on the basis of HFE.\r\n   In particular, we use our attack to show that the proposed parameters of the GeMSS scheme are not as secure as claimed.",
                  "authors": [
                    "Chengdong Tao",
                    "Albrecht Petzoldt",
                    "Jintai Ding"
                  ],
                  "affiliations": "Beijing Institute of Mathematical Sciences and Applications; FAU Erlangen-Nuremberg; Yau Mathematical Center, Tsinghua University",
                  "category": "Uncategorized",
                  "id": "talk-70"
                },
                {
                  "paperId": "84",
                  "title": "Three Halves Make a Whole? Beating the Half-Gates Lower Bound for Garbled Circuits",
                  "abstract": "We describe a garbling scheme for boolean circuits, in which XOR gates are free and AND gates require communication of $1.5\\kappa + 5$ bits. This improves over the state-of-the-art ``half-gates'' scheme of Zahur, Rosulek, and Evans (Eurocrypt 2015), in which XOR gates are free and AND gates cost $2\\kappa$ bits. The half-gates paper proved a lower bound of $2\\kappa$ bits per AND gate, in a model that captured all known garbling techniques at the time. We bypass this lower bound with a novel technique that we call \\textbf{slicing and dicing}, which involves slicing wire labels in half and operating separately on those halves. Ours is the first to bypass the lower bound while being fully compatible with free-XOR, making it a drop-in replacement for half-gates. Our construction is proven secure from a similar assumption to prior free-XOR garbling (circular correlation-robust hash), and uses only slightly more computation than half-gates.",
                  "authors": [
                    "Mike Rosulek",
                    "Lawrence Roy"
                  ],
                  "affiliations": "Oregon State University",
                  "category": "Uncategorized",
                  "id": "talk-48"
                }
              ],
              "id": "session-8"
            }
          ]
        },
        {
          "starttime": "16:00",
          "endtime": "16:30",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-9"
            }
          ]
        },
        {
          "starttime": "16:30",
          "endtime": "17:20",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Lattice Cryptography",
              "talks": [
                {
                  "paperId": "91",
                  "title": "Subtractive Sets over Cyclotomic Rings: Limits of Schnorr-like Arguments over Lattices",
                  "abstract": "We study when (dual) Vandermonde systems of the form `V_T ⋅ z = s⋅w` admit a solution `z` over a ring `R`, where `V_T` is the Vandermonde matrix defined by a set `T` and where the “slack” `s` is a measure of the quality of solutions. To this end, we propose the notion of `(s,t)`-subtractive sets over a ring `R`, with the property that if `S` is `(s,t)`-subtractive then the above (dual) Vandermonde systems defined by any `t`-subset `T ⊆ S` are solvable over `R`. The challenge is then to find large sets `S` while minimising (the norm of) `s` when given a ring `R`.\r\n\r\nBy constructing families of `(s,t)`-subtractive sets `S` of size `n = poly(λ)` over cyclotomic rings `R = ZZ[ζ_{p^ℓ}]` for prime `p`, we construct Schnorr-like lattice-based proofs of knowledge for the SIS relation `A ⋅ x = s ⋅ y mod q` with `O(1/n)` knowledge error, and `s=1` in case `p = poly(λ)`. Our technique slots naturally into the lattice Bulletproof framework from Crypto’20, producing lattice-based succinct arguments for NP with better parameters.\r\n\r\nWe then give matching impossibility results constraining `n` relative to `s`, which suggest that our Bulletproof-compatible protocols are optimal unless fundamentally new techniques are discovered. Noting that the knowledge error of lattice Bulletproofs is `Ω(log k/n)` for witnesses in `R^k` and subtractive set size `n`, our result represents a barrier to practically efficient lattice-based succinct arguments in the Bulletproof framework.\r\n\r\nBeyond these main results, the concept of `(s,t)`-subtractive sets bridges group-based threshold cryptography to the lattice settings, which we demonstrate by relating it to distributed pseudorandom functions.",
                  "authors": [
                    "Martin Albrecht",
                    "Russell W. F. Lai"
                  ],
                  "affiliations": "Royal Holloway, University of London; Chair of Applied Cryptography, Friedrich-Alexander-Universität Erlangen-Nürnberg",
                  "category": "Uncategorized",
                  "id": "talk-49"
                },
                {
                  "paperId": "420",
                  "title": "A Compressed Sigma-Protocol Theory for Lattices",
                  "abstract": "We show a {\\em lattice-based} solution for commit-and-prove transparent circuit ZK with {\\em polylog-communication}, the {\\em first} not depending on PCPs.\r\n\r\nWe start from {\\em compressed $\\Sigma$-protocol theory} (CRYPTO 2020), which is built around basic $\\Sigma$-protocols for opening an arbitrary linear form on a long secret vector that is\r\ncompactly committed to. These protocols are first compressed using a recursive ``folding-technique'' adapted from Bulletproofs, at the expense of logarithmic rounds. Proving in ZK that the secret vector satisfies a given constraint -- captured by a circuit -- is then by  (blackbox) reduction to the linear case, via arithmetic secret-sharing techniques adapted from MPC. Commit-and-prove is also facilitated, i.e., when commitment(s) to the secret vector are created ahead of any circuit-ZK proof. On several platforms (incl.\\ DL) this leads to logarithmic communication. \r\nNon-interactive versions follow from Fiat-Shamir.\r\n\r\nThis abstract modular theory strongly suggests that it should somehow be supported by a lattice-platform {\\em as well}. However, when going through the motions and trying to establish low communication (on an SIS-platform), a certain significant lack in current understanding of multi-round protocols is exposed.\r\n\r\nNamely, as opposed to the DL-case,  the basic $\\Sigma$-protocol in question has {\\em poly-small challenge} space. Taking into account the compression-step -- which yields {\\em non-constant} rounds -- and the necessity for parallelization to reduce error, there is no known result that the compound protocol admits an efficient knowledge extractor with small error. In fact, we identify a recent related work where this accounts for a gap in the analysis. We resolve the state of affairs here by a combination of two novel results which are fully general and of independent interest.\r\nThe first gives a tight analysis of efficient knowledge extraction in case of non-constant rounds combined with poly-small challenge space, whereas the  second shows that parallel repetition indeed forces rapid decrease of knowledge error. \r\n\r\nMoreover, in our present context, arithmetic secret sharing is not defined over a large finite field but over a quotient of a number ring and this forces our careful adaptation of how the linearization techniques are deployed.\r\n\r\nWe develop our protocols in an abstract framework that is conceptually simple and can be flexibly instantiated. In particular, the framework applies to arbitrary rings and norms. \r\n\r\nAs a byproduct, our compressed $\\Sigma$-protocol can double as a PoK for an SIS preimage. \r\nIn this mode of operation, it improves the communication-efficiency of the PoK by Bootle et al.\\ (CRYPTO 2020).",
                  "authors": [
                    "Thomas Attema",
                    "Ronald Cramer",
                    "Lisa Kohl"
                  ],
                  "affiliations": "CWI & TNO; CWI & Leiden University; CWI",
                  "category": "Uncategorized",
                  "id": "talk-130"
                },
                {
                  "paperId": "193",
                  "title": "A New Simple Technique to Bootstrap Various Lattice Zero-Knowledge Proofs to QROM Secure NIZKs",
                  "abstract": "Many of the recent advanced lattice-based Sigma-/public-coin honest verifier (HVZK) interactive protocols based on the techniques developed by Lyubashevsky (Asiacrypt'09, Eurocrypt'12) can be transformed into a non-interactive zero-knowledge (NIZK) proof in the random oracle model (ROM) using the Fiat-Shamir transform. Unfortunately, although they are known to be secure in the __classical__ ROM, existing proof techniques are incapable of proving them secure in the __quantum__ ROM (QROM). Alternatively, while we could instead rely on the Unruh transform (Eurocrypt'15), the resulting QROM secure NIZK will incur a large overhead compared to the underlying interactive protocol. \r\n\r\nIn this paper, we present a new simple semi-generic transform that compiles many existing lattice-based Sigma-/public-coin HVZK interactive protocols into QROM secure NIZKs. \r\nOur transform builds on a new primitive called __extractable linear homomorphic commitment__ protocol. The resulting NIZK has several appealing features: it is not only a proof of knowledge but also straight-line extractable; the proof overhead is smaller compared to the Unruh transform; it enjoys a relatively small reduction loss; and it requires minimal background on quantum computation. To illustrate the generality of our technique, we show how to transform the recent Bootle et al.'s 5-round protocol with an exact sound proof (Crypto'19) into a QROM secure NIZK by increasing the proof size by a factor of 2.6. This compares favorably to the Unruh transform that requires a factor of more than 50.",
                  "authors": [
                    "Shuichi Katsumata"
                  ],
                  "affiliations": "AIST, Japan",
                  "category": "Uncategorized",
                  "id": "talk-81"
                },
                {
                  "paperId": "384",
                  "title": "SMILE: Set Membership from Ideal Lattices with Applications to Ring Signatures and Confidential Transactions",
                  "abstract": "In a set membership proof, the public information consists of a set of elements and a commitment. The prover then produces a zero-knowledge proof showing that the commitment is indeed to some element from the set. This primitive is closely related to concepts like ring signatures and ``one-out-of-many'' proofs that underlie many anonymity and privacy protocols. The main result of this work is a new succinct lattice-based set membership proof whose size is logarithmic in the size of the set. \r\n\r\nWe also give transformations of our set membership proof to a ring signature scheme and to a confidential transaction payment system. The ring signature size is also logarithmic in the size of the public key set and has size $16$~KB for a set of $2^5$ elements, and $22$~KB for a set of size $2^{25}$. At an approximately $128$-bit security level, these outputs are between 1.5X and 7X smaller than the current state of the art succinct ring signatures of Beullens et al. (Asiacrypt 2020) and Esgin et al. (CCS 2019).  \r\n\r\nWe then show that our ring signature, combined with a few other techniques and optimizations, can be turned into a fairly efficient Monero-like confidential transaction system based on the MatRiCT framework of Esgin et al. (CCS 2019).  With our new techniques, we are able to reduce the transaction proof size by factors of about 4X - 10X over the aforementioned work. For example, a transaction with two inputs and two outputs, where each input is hidden among $2^{15}$ other accounts, requires approximately $30$KB in our protocol.",
                  "authors": [
                    "Vadim Lyubashevsky",
                    "Ngoc Khanh Nguyen",
                    "Gregor Seiler"
                  ],
                  "affiliations": "IBM Research Europe - Zurich; IBM Research Europe - Zurich & ETH Zurich",
                  "category": "Uncategorized",
                  "id": "talk-121"
                },
                {
                  "paperId": "219",
                  "title": "Deniable Fully Homomorphic Encryption from Learning With Errors",
                  "abstract": "We define and construct {\\it Deniable Fully Homomorphic Encryption} based on the Learning With Errors (LWE) polynomial hardness assumption. Deniable FHE enables storing encrypted data in the cloud to be processed securely without decryption, maintaining deniability of the encrypted data, as well the prevention of vote-buying in electronic voting schemes where encrypted votes can be tallied without decryption.\r\n\r\nOur constructions achieve {\\it compactness} independently of the level of deniability- both the size of the public key and the size of the ciphertexts are bounded by a fixed polynomial, independent of the faking probability achieved by the scheme. This is in contrast to all previous constructions of deniable encryption schemes (even without requiring homomorphisms) which are based on polynomial hardness assumptions, originating with the seminal work of Canetti, Dwork, Naor and Ostrovsky (CRYPTO 1997) in which the ciphertext size grows with the inverse of the faking probability. Canetti {\\it et al.} argued that this dependence ``seems inherent'', but our constructions illustrate this is not the case. We note that the Sahai-Waters (STOC13) construction of deniable encryption from indistinguishability-obfuscation achieves compactness and can be easily modified to achieve deniable FHE as well, but it requires multiple, stronger sub-exponential hardness assumptions, which are furthermore not post-quantum secure. In contrast, our constructions rely only on the LWE polynomial hardness assumption, as currently required for FHE even without deniability.\r\n\r\nThe running time of our encryption algorithm depends on the inverse of the faking probability, thus the scheme falls short of achieving simultaneously compactness, negligible deniability probability {\\it and} polynomial encryption time. Yet, we believe that achieving compactness is a fundamental step on the way to achieving all properties simultaneously as has been the historical journey for other primitives such as functional encryption. Interestingly, we note that our constructions support large message spaces, whereas previous constructions were bit by bit, and can be run in online-offline model of encryption, where the bulk of computation is independent of the message and may be performed in an offline pre-processing phase. The running time of the online phase, is independent of the faking probability, whereas the offline encryption run-time grows with the inverse of the faking probability. \r\n\r\nAt the heart of our constructions is a new way to use bootstrapping to obliviously generate FHE ciphertexts so that it supports faking under coercion.",
                  "authors": [
                    "Shweta Agrawal",
                    "Shafi Goldwasser",
                    "Saleet Mossel"
                  ],
                  "affiliations": "IIT Madras; Simons Institute of TOC; MIT",
                  "category": "Uncategorized",
                  "id": "talk-87"
                }
              ],
              "id": "session-10"
            }
          ]
        },
        {
          "starttime": "17:20",
          "endtime": "17:50",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-11"
            }
          ]
        },
        {
          "starttime": "17:50",
          "endtime": "18:40",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Lattice Cryptanalysis",
              "talks": [
                {
                  "paperId": "334",
                  "title": "Counterexamples to Circular Security-Based iO",
                  "abstract": "We study several strenthenings of classical circular security assumptions which were recently introduced in four new constructions of indistinguishabilty obfuscation from lattice-based primitives: Brakerski-D\\\"ottling-Garg-Malavolta (Eurocrypt 2020), Gay-Pass (STOC 2021), Brakerski-D\\\"ottling-Garg-Malavolta (Eprint 2020) and Wee-Wichs (Eprint 2020).\r\n  \r\nWe provide explicit counterexamples to the {\\em $2$-circular shielded randomness leakage} assumption of Gay-Pass and the {\\em homomorphic pseudorandom LWE samples} conjecture of Wee-Wichs.\r\nOur work suggests a separation between classical circular security of the kind underlying un-levelled fully-homomorphic encryption from the strengthened versions underlying recent iO constructions, showing that they are not (yet) on the same rigorous footing.\r\n  \r\n Our counterexamples exploit the flexibility to choose specific implementations of circuits and LWE distributions in the Gay-Pass and Wee-Wichs assumptions. It remains possible that other implementations do result in an unbroken $\\iO$ scheme. But our work shows that this will, at least, require refinement of the assumptions. In particular, generic circular security assumptions/definitions are insufficient, and their security is actually sensitive to the specific structure of the leakages involved.",
                  "authors": [
                    "Sam Hopkins",
                    "Aayush Jain",
                    "Huijia Lin"
                  ],
                  "affiliations": "UC Berkeley; UCLA; UW",
                  "category": "Uncategorized",
                  "id": "talk-106"
                },
                {
                  "paperId": "226",
                  "title": "How to Meet Ternary LWE Keys",
                  "abstract": "The LWE problem with its ring variants is today the most prominent candidate for building efficient public key cryptosystems resistant to quantum computers. NTRU-type cryptosystems use an LWE-type variant with small max-norm secrets, usually with ternary coefficients from the set $\\{-1,0,1\\}$. The presumably best attack on these schemes is a hybrid attack that combines lattice reduction techniques with Odlyzko's Meet-in-the-Middle approach. Odlyzko's algorithm is a classical combinatorial attack that for key space size $S$ runs in time $S^{0.5}$. We substantially improve on this Meet-in-the-Middle approach, using the representation technique developed for subset sum  algorithms. Asymptotically, our heuristic Meet-in-the-Middle attack runs in time roughly $S^{0.25}$, which also beats the $S^{\\frac 1 3}$ complexity of the best known quantum algorithm. \r\n\r\nFor the round-3 NIST post-quantum encryptions NTRU-Encrypt and NTRU-Prime we obtain non-asymptotic instantiations of our  attack with complexity  roughly $S^{0.35}$. \r\nAs opposed to other combinatorial attacks, our attack benefits from larger LWE field sizes $q$, as they are often used in modern lattice-based signatures. For example, for BLISS signatures we obtain non-asymptotic combinatorial attacks in between $S^{0.31}$ and $S^{0.35}$, for GLP signatures in $S^{0.3}$. \r\n\r\nOur attacks do not invalidate the security claims of the aforementioned schemes. However, they establish improved combinatorial upper bounds for their security. We leave it is an open question whether our new Meet-in-the-Middle attack in combination with lattice reduction can be used to speed up the hybrid attack.",
                  "authors": [
                    "Alexander May"
                  ],
                  "affiliations": "Ruhr University Bochum",
                  "category": "Uncategorized",
                  "id": "talk-89"
                },
                {
                  "paperId": "74",
                  "title": "Lattice Reduction with Approximate Enumeration Oracles: Practical Algorithms and Concrete Performance",
                  "abstract": "This work provides a systematic investigation of the use of approximate enumeration oracles in BKZ, building on recent technical progress on speeding-up lattice enumeration: relaxing (the search radius of) enumeration and extended preprocessing which preprocesses in a larger rank than the enumeration rank. First, we heuristically justify that relaxing enumeration with certain extreme pruning asymptotically achieves an exponential speed-up for reaching the same root Hermite factor (RHF). Second, we perform simulations/experiments to validate this and the performance for relaxed enumeration with numerically optimised pruning for both regular and extended preprocessing. \r\nUpgrading BKZ with such approximate enumeration oracles gives rise to our main result, namely a practical and faster (compared to previous work) polynomial-space lattice reduction algorithm for reaching the same RHF in practical and cryptographic parameter ranges. We assess its concrete time/quality performance with extensive simulations and experiments. As a consequence, we update the extrapolation of the crossover rank between a square-root cost estimate for quantum enumeration using our algorithm and the Core-SVP cost estimate for quantum sieving to 547.",
                  "authors": [
                    "Martin Albrecht",
                    "Shi Bai",
                    "Jianwei Li",
                    "Joe Rowell"
                  ],
                  "affiliations": "Royal Holloway, University of London; Florida Atlantic University",
                  "category": "Uncategorized",
                  "id": "talk-44"
                },
                {
                  "paperId": "302",
                  "title": "Towards faster polynomial-time lattice reduction",
                  "abstract": "The LLL algorithm is a polynomial-time algorithm for reducing d-dimensional lattice with exponential approximation factor. Currently, the most efficient variant of LLL, by Neumaier and Stehl\\'e, has a theoretical running time in $d^4\\cdot B^{1+o(1)}$ where $B$ is the bitlength of the\r\nentries, but has never been implemented. This work introduces new asymptotically fast, parallel, yet heuristic, reduction algorithms with their optimized implementations. Our algorithms are recursive and fully exploit fast block matrix multiplication. We experimentally demonstrate that by carefully controlling the floating-point precision during the recursion steps, we can reduce euclidean lattices of rank d in time $\\tilde{O}(d^\\omega\\cdot C)$, i.e., almost a constant number of matrix multiplications, where $\\omega$ is the exponent of matrix multiplication and C is the log of the condition number of the matrix. For cryptographic applications, C is close to B, while it can be up to d times larger in the worst case. It improves the running-time of the state-of-the-art implementation fplll by a multiplicative factor of order $d^2\\cdot B$. Further, we show that we can reduce structured lattices, the so-called knapsack lattices, in time $\\tilde{O}(d^{\\omega-1}\\cdot C)$ with a progressive reduction strategy. Besides allowing reducing huge lattices, our implementation can break several instances of Fully Homomorphic Encryption schemes based \r\non large integers in dimension 2,230 with 4 millions of bits.",
                  "authors": [
                    "Paul Kirchner",
                    "Thomas Espitau",
                    "Pierre-Alain Fouque"
                  ],
                  "affiliations": "University Rennes; NTT Research and Development",
                  "category": "Uncategorized",
                  "id": "talk-99"
                },
                {
                  "paperId": "201",
                  "title": "Lower bounds on lattice sieving and information set decoding",
                  "abstract": "In two of the main areas of post-quantum cryptography, based on lattices and codes, nearest neighbor techniques have been used to speed up state-of-the-art cryptanalytic algorithms, and to obtain the lowest asymptotic cost estimates to date [May--Ozerov, Eurocrypt'15; Becker--Ducas--Gama--Laarhoven, SODA'16]. These upper bounds are useful for assessing the security of cryptosystems against known attacks, but to guarantee long-term security one would like to have closely matching lower bounds, showing that improvements on the algorithmic side will not drastically reduce the security in the future. As existing lower bounds from the nearest neighbor literature do not apply to the nearest neighbor problems appearing in this context, one might wonder whether further speedups to these cryptanalytic algorithms can still be found by only improving the nearest neighbor subroutines.\r\nWe derive new lower bounds on the costs of solving the nearest neighbor search problems appearing in these cryptanalytic settings. For the Euclidean metric we show that for random data sets on the sphere, the locality-sensitive filtering approach of [Becker--Ducas--Gama--Laarhoven, SODA 2016] using spherical caps is optimal, and hence within a broad class of lattice sieving algorithms covering almost all approaches to date, their asymptotic time complexity of $2^{0.292d + o(d)}$ is optimal. Similar conditional optimality results apply to lattice sieving variants, such as the $2^{0.265d + o(d)}$ complexity for quantum sieving [Laarhoven, PhD thesis 2016] and previously derived complexity estimates for tuple sieving [Herold--Kirshanova--Laarhoven, PKC 2018]. For the Hamming metric we derive new lower bounds for nearest neighbor searching which almost match the best upper bounds from the literature [May--Ozerov, Eurocrypt 2015]. As a consequence we derive conditional lower bounds on decoding attacks, showing that also here one should search for improvements elsewhere to significantly undermine security estimates from the literature.",
                  "authors": [
                    "Thijs Laarhoven",
                    "Elena Kirshanova"
                  ],
                  "affiliations": "Eindhoven University of Technology; Immanuel Kant Baltic Federal University",
                  "category": "Uncategorized",
                  "id": "talk-84"
                }
              ],
              "id": "session-12"
            }
          ]
        },
        {
          "starttime": "18:40",
          "endtime": "19:40",
          "sessions": [
            {
              "id": "session-143",
              "session_title": "Social Break "
            }
          ]
        },
        {
          "starttime": "19:40",
          "endtime": "20:10",
          "sessions": [
            {
              "id": "session-144",
              "session_title": "Multi-Party Computation 2",
              "talks": [
                {
                  "paperId": "81",
                  "title": "Broadcast-Optimal Two Round MPC with an Honest Majority",
                  "abstract": "This paper closes the question of the possibility of two-round MPC protocols achieving different security guarantees with and without the availability of broadcast in any given round. Cohen et al. (Eurocrypt 2020) study this question in the dishonest majority setting; we complete the picture by studying the honest majority setting.\r\n\r\nIn the honest majority setting, given broadcast in both rounds, it is known that the strongest guarantee — guaranteed output delivery — is achievable (Gordon et al. Crypto 2015). We show that, given broadcast in the first round only, guaranteed output delivery is still achievable. Given broadcast in the second round only, we give a new construction that achieves identifiable abort, and we show that fairness — and thus guaranteed output delivery — are not achievable in this setting. Finally, if only peer-to-peer channels are available, we show that the weakest guarantee — selective abort — is the only one achievable for corruption thresholds t > 1 and for t = 1 and n = 3. On the other hand, it is already known that selective abort can be achieved in these cases. In the remaining cases, i.e., t = 1 and n > 3, it is known (from the work of Ishai et al. at Crypto 2010, and Ishai et al. at Crypto 2015) that guaranteed output delivery (and thus all weaker guarantees) are possible.",
                  "authors": [
                    "Ivan Damgård",
                    "Bernardo Magri",
                    "Divya Ravi",
                    "Luisa Siniscalchi",
                    "Sophia Yakoubov"
                  ],
                  "affiliations": "Aarhus University",
                  "category": "Uncategorized",
                  "id": "talk-46"
                },
                {
                  "paperId": "200",
                  "title": "Three-Round Secure Multiparty Computation from Black-Box Two-Round Oblivious Transfer",
                  "abstract": "We give constructions of three-round secure multiparty computation (MPC) protocols for general functions that make {\\it black-box} use of a two-round oblivious transfer (OT). For the case of semi-honest adversaries, we make use of a two-round, semi-honest secure OT in the plain model. This resolves the round-complexity of black-box (semi-honest) MPC protocols from minimal assumptions and answers an open question of Applebaum et al. (ITCS 2020). For the case of malicious adversaries, we make use of a two-round maliciously-secure OT in the common random/reference string model that satisfies a (mild) variant of adaptive security for the receiver.",
                  "authors": [
                    "Arpita Patra",
                    "Akshayaram Srinivasan"
                  ],
                  "affiliations": "Indian Institute of Science; Tata Institute of Fundamental Research",
                  "category": "Uncategorized",
                  "id": "talk-83"
                },
                {
                  "paperId": "205",
                  "title": "On the Round Complexity of Black-Box Secure MPC",
                  "abstract": "We consider the question of minimizing the round complexity of secure multiparty computation (MPC) protocols that make a black-box use of simple cryptographic primitives in the setting of security against any number of malicious parties. In the plain model, previous black-box protocols required a high constant number of rounds (>15). This is far from the known lower bound of 4 rounds for protocols with black-box simulators. \r\n\r\nWhen allowing a random oblivious transfer (OT) correlation setup, 2-round protocols making a black-box use of a pseudorandom generator were previously known. However, such protocols were obtained via a round-collapsing ``protocol garbling'' technique that has poor concrete efficiency and makes a non-black-box use of an underlying malicious-secure protocol.\r\n\r\nWe improve this state of affairs by presenting the following types of black-box protocols. \r\n\r\na. 4-round ``pairwise MPC'' in the plain model. \r\nThis round-optimal protocol enables each ordered pair of parties to compute a function of both inputs whose output is delivered to the second party. The protocol makes black-box use of any public-key encryption (PKE) with pseudorandom public keys. As a special case, we get a  black-box round-optimal realization of secure (copies of) OT between every ordered pair of parties.\r\n    \r\nb. 2-round MPC from OT correlations. \r\nThis round-optimal protocol makes a black-box use of any general 2-round MPC protocol satisfying an augmented notion of semi-honest security. In the two-party case, this yields new kinds of 2-round black-box protocols.  \r\n    \r\nc. 5-round MPC in the plain model. \r\nThis protocol makes a black-box use of PKE with pseudorandom public keys, and 2-round oblivious transfer with ``semi-malicious'' security. \r\n\r\nA key technical tool for the first result is a novel combination of split-state non-malleable codes (Dziembowski, Pietrzak, and Wichs, JACM '18) with standalone secure {\\em two-party} protocols. The second result is based on a new round-optimized variant of the ``IPS compiler'' (Ishai, Prabhakaran and Sahai, Crypto '08). The third result is obtained via a specialized combination of these two techniques.",
                  "authors": [
                    "Yuval Ishai",
                    "Dakshita Khurana",
                    "Amit Sahai",
                    "Akshayaram Srinivasan"
                  ],
                  "affiliations": "Technion; UIUC; UCLA; Tata Institute of Fundamental Research",
                  "category": "Uncategorized",
                  "id": "talk-86"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "20:10",
          "endtime": "20:20",
          "sessions": [
            {
              "id": "session-145",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "20:20",
          "endtime": "21:00",
          "sessions": [
            {
              "id": "session-146",
              "session_title": "Multi-Party Computation 2 (continued)",
              "talks": [
                {
                  "paperId": "145",
                  "title": "ATLAS: Efficient and Scalable MPC in the Honest Majority Setting",
                  "abstract": "In this work, we address communication, computation, and round efficiency of unconditionally secure multi-party computation for arithmetic circuits in the honest majority setting. We achieve both algorithmic and practical improvements: \r\n\r\n - The best known result in the semi-honest setting has been due to Damgard and Nielsen (CRYPTO 2007). Over the last decade, their construction has played an important role in the progress of efficient secure computation. However despite a number of follow-up works, any significant improvements to the basic semi-honest protocol have been hard to come by. We show 33% improvement in communication complexity of this protocol. We show how to generalize this result to the malicious setting, leading to the best known unconditional honest majority MPC with malicious security. \r\n \r\n - We focus on the round complexity of the Damgard and Nielsen protocol and improve it by a factor of 2. Our improvement relies on a novel observation relating to an interplay between Damgard and Nielsen multiplication and Beaver triple multiplication. An implementation of our constructions shows an execution run time improvement compared to the state of the art ranging from 30% to 50%.",
                  "authors": [
                    "Vipul Goyal",
                    "Hanjun Li",
                    "Rafail Ostrovsky",
                    "Antigoni Polychroniadou",
                    "Yifan Song"
                  ],
                  "affiliations": "CMU and NTT Research; University of Washington; UCLA; J.P. Morgan AI Research; Carnegie Mellon University",
                  "category": "Uncategorized",
                  "id": "talk-62"
                },
                {
                  "paperId": "412",
                  "title": "Unconditional Communication-Efficient MPC via Hall's Marriage Theorem",
                  "abstract": "The best known n party unconditional multiparty computation protocols with an optimal corruption threshold communicates O(n) field elements per gate. This has been the case even in the semi-honest setting despite over a decade of research on communication complexity in this setting. Going to the slightly sub-optimal corruption setting, the work of Damgard, Ishai, and Kroigaard (EUROCRYPT 2010) provided the first protocol for a single circuit achieving communication complexity of O(log|C| * n/k) elements per gate. While a number of works have improved upon this result, obtaining a protocol with O(1) field elements per gate has been an open problem.\r\n\r\nIn this work, we construct the first unconditional multi-party computation protocol evaluating a single arithmetic circuit with amortized communication complexity of O(n/k) or O(1) elements per gate for k=\\Omega(n) with corruption threshold t'=(n-1)/2-k+1.",
                  "authors": [
                    "Vipul Goyal",
                    "Antigoni Polychroniadou",
                    "Yifan Song"
                  ],
                  "affiliations": "CMU and NTT Research; J.P. Morgan AI Research; Carnegie Mellon University",
                  "category": "Uncategorized",
                  "id": "talk-127"
                },
                {
                  "paperId": "107",
                  "title": "Non-Interactive Secure Multiparty Computation for Symmetric Functions, Revisited: More Efficient Constructions and Extensions",
                  "abstract": "Non-interactive secure multiparty computation (NIMPC) is a variant of secure computation which allows each of $n$ players to send only a single message depending on his input and correlated randomness.\r\nAbelian programs, which can realize any symmetric function, are defined as functions on the sum of the players' inputs over an abelian group and provide useful functionalities for real-world applications.\r\nWe improve and extend the previous results in the following ways:\r\n\\begin{itemize}\r\n\\item We present NIMPC protocols for abelian programs that improve the best known communication complexity.\r\nIf inputs take any value of an abelian group $\\mathbb{G}$, our protocol achieves the communication complexity $O(|\\mathbb{G}|(\\log|\\mathbb{G}|)^2)$ improving $O(|\\mathbb{G}|^2n^2)$ of Beimel et al. (Crypto 2014).\r\nIf players are limited to inputs from subsets of size at most $d$, our protocol achieves $|\\mathbb{G}|(\\log|\\mathbb{G}|)^2(\\max\\{n,d\\})^{(1+o(1))t}$ where $t$ is a corruption threshold.\r\nThis result improves $|\\mathbb{G}|^3(nd)^{(1+o(1))t}$ of Beimel et al. (Crypto 2014), and even $|\\mathbb{G}|^{\\log n+O(1)}n$ of Benhamouda et al. (Crypto 2017) if $t=o(\\log n)$ and $|\\mathbb{G}|=n^{\\Theta(1)}$.\r\n\r\n\\item We propose for the first time NIMPC protocols for linear classifiers that are more efficient than those obtained from the generic construction.\r\n\r\n\\item We revisit a known transformation of Benhamouda et al. (Crypto 2017) from Private Simultaneous Messages (PSM) to NIMPC, which we repeatedly use in the above results.\r\nWe reveal that a sub-protocol used in the transformation does not satisfy the specified security.\r\nWe also fix their protocol with only constant overhead in the communication complexity.\r\nAs a byproduct, we obtain an NIMPC protocol for indicator functions with asymptotically optimal communication complexity with respect to the input length.\r\n\\end{itemize}",
                  "authors": [
                    "Reo Eriguchi",
                    "Kazuma Ohara",
                    "Shota Yamada",
                    "Koji Nuida"
                  ],
                  "affiliations": "The University of Tokyo / AIST; AIST; Kyushu University / AIST",
                  "category": "Uncategorized",
                  "id": "talk-54"
                },
                {
                  "paperId": "135",
                  "title": "Efficient Information-Theoretic Multi-Party Computation over Non-Commutative Rings",
                  "abstract": "We construct the first efficient MPC protocol that only requires black-box access to a non-commutative ring $R$.\r\nPrevious results in the same setting were efficient only either for a constant number of corruptions or when computing branching programs and formulas.\r\nOur techniques are based on a generalization of Shamir's secret sharing to non-commutative rings, which we derive from the work on Reed Solomon codes by Quintin, Barbier and Chabot (\\textit{IEEE Transactions on Information Theory, 2013}).\r\nWhen the center of the ring contains a set $A = \\{\\alpha_0, \\ldots, \\alpha_n\\}$ such that $\\forall i \\neq j, \\alpha_i - \\alpha_j \\in R^*$, the resulting secret sharing scheme is strongly multiplicative and we can generalize existing constructions over finite fields without much trouble.\r\n\r\nMost of our work is devoted to the case where the elements of $A$ do not commute with all of $R$, but they just commute with each other.\r\nFor such rings, the secret sharing scheme cannot be linear ``on both sides\" and furthermore it is not multiplicative. Nevertheless, we are still able to build MPC protocols with a concretely efficient online phase and black-box access to $R$. As an example we consider the ring $\\mathcal{M}_{m\\times m}(\\mathbb{Z}/2^k\\mathbb{Z})$, for which when $m > \\log(n+1)$, \\enote{maybe adapt/simplify the following claim as the comparison requires some nuances} we obtain protocols that require around $\\lceil\\log(n+1)\\rceil/2$ less communication and $2\\lceil\\log(n+1)\\rceil$ less computation than the state of the art protocol based on  Circuit Amortization Friendly Encodings (Dalskov, Lee and Soria-Vazquez, \\textit{ASIACRYPT 2020}).\r\n\r\nIn this setting with a ``less commutative\" $A$, our black-box preprocessing phase has a less practical complexity of $\\poly(n)$. Due to this, we additionally provide specialized, concretely efficient preprocessing protocols for $R = \\mathcal{M}_{m\\times m}(\\mathbb{Z}/2^k\\mathbb{Z})$ that exploit the structure of the matrix ring.",
                  "authors": [
                    "Daniel Escudero",
                    "Eduardo Soria-Vazquez"
                  ],
                  "affiliations": "Aarhus University",
                  "category": "Uncategorized",
                  "id": "talk-58"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "date": "2021-08-18",
      "timeslots": [
        {
          "starttime": "15:00",
          "endtime": "15:40",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Quantum Cryptography 2",
              "talks": [
                {
                  "paperId": "136",
                  "title": "New Approaches for Quantum Copy-Protection",
                  "abstract": "Quantum copy protection uses the unclonability of quantum states to construct quantum software that provably cannot be pirated. Copy protection would be immensely useful, but unfortunately little is known about how to achieve it in general. In this work, we make progress on this goal, by giving the following results: \r\n\r\n* We show how to copy protect any program that cannot be learned from its input/output behavior, relative to a classical oracle. This improves on Aaronson (CCC 2009), which achieves the same relative to a quantum oracle. By instantiating the oracle with post-quantum candidate obfuscation schemes, we obtain a heuristic construction of copy protection. \r\n\r\n* We show, roughly, that any program which can be watermarked can be copy detected, a weaker version of copy protection that does not prevent copying, but guarantees that any copying can be detected. Our scheme relies on the security of the assumed watermarking, plus the assumed existence of public key quantum money. Our construction is general, applicable to many recent watermarking schemes.",
                  "authors": [
                    "Scott Aaronson",
                    "Jiahui Liu",
                    "Qipeng Liu",
                    "Mark Zhandry",
                    "Ruizhe Zhang"
                  ],
                  "affiliations": "The University of Texas at Austin; Princeton University; Princeton University and NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-59"
                },
                {
                  "paperId": "286",
                  "title": "Hidden Cosets and Applications to Unclonable Cryptography",
                  "abstract": "In 2012, Aaronson and Christiano introduced the idea of hidden subspace states to build public-key quantum money [STOC '12]. Since then, this idea has been applied to realize several other cryptographic primitives which enjoy some form of unclonability.\r\n    \r\nIn this work, we propose a generalization of hidden subspace states to hidden coset states. We study different unclonable properties of coset states and several applications:\r\n\r\n* We show that, assuming indistinguishability obfuscation (iO), hidden coset states possess a certain direct product hardness property, which immediately implies a tokenized signature scheme in the plain model. Previously, a tokenized signature scheme was known only relative to an oracle, from a work of Ben-David and Sattath [QCrypt '17].\r\n        \r\n* Combining a tokenized signature scheme with extractable witness encryption, we give a construction of an unclonable decryption scheme in the plain model. The latter primitive was recently proposed by Georgiou and Zhandry [ePrint '20], who gave a construction relative to a classical oracle.\r\n        \r\n* We conjecture that coset states satisfy a certain natural monogamy-of-entanglement property. Assuming this conjecture is true, we remove the requirement for extractable witness encryption in our unclonable decryption construction. As potential evidence in support of the conjecture, we prove a weaker version of this monogamy property, which we believe will still be of independent interest. \r\n\r\n* Finally, we give the first construction of a copy-protection scheme for pseudorandom functions (PRFs) in the plain model. Our scheme is secure either assuming iO and extractable witness encryption, or iO, LWE and the conjectured monogamy property mentioned above. This is the first example of a copy-protection scheme with provable security in the plain model for a class of functions that is not evasive.",
                  "authors": [
                    "Andrea Coladangelo",
                    "Jiahui Liu",
                    "Qipeng Liu",
                    "Mark Zhandry"
                  ],
                  "affiliations": "University of California, Berkeley; The University of Texas at Austin; Princeton University; Princeton University and NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-97"
                },
                {
                  "paperId": "169",
                  "title": "On Tight Quantum Security of HMAC and NMAC in the Quantum Random Oracle Model",
                  "abstract": "HMAC and NMAC are the most basic and important constructions to convert Merkle-Damg{\\aa}rd hash functions into message authentication codes (MACs) or pseudorandom functions (PRFs).\r\nIn the quantum setting, at CRYPTO~2017, Song and Yun showed that HMAC and NMAC are quantum pseudorandom functions (qPRFs) under the standard assumption that the underlying compression function is a qPRF.\r\nTheir proof guarantees security up to $O(2^{n/5})$ or $O(2^{n/8})$ quantum queries when the output length of HMAC and NMAC is $n$ bits.\r\nHowever, there is a gap between the provable security bound and a simple distinguishing attack that uses $O(2^{n/3})$ quantum queries. \r\nThis paper settles the problem of closing the gap.\r\nWe show that the tight bound of the number of\r\nquantum queries to distinguish HMAC or NMAC from a random function\r\nis $\\Theta(2^{n/3})$ in the quantum random oracle model,\r\nwhere compression functions are modeled as quantum random oracles.\r\nTo give the tight quantum bound,\r\nbased on an alternative formalization of Zhandry's compressed oracle technique,\r\nwe introduce a new proof technique focusing on the symmetry of quantum query records.",
                  "authors": [
                    "Akinori Hosoyamada",
                    "Tetsu Iwata"
                  ],
                  "affiliations": "NTT Corporation and Nagoya University; Nagoya University",
                  "category": "Uncategorized",
                  "id": "talk-71"
                },
                {
                  "paperId": "183",
                  "title": "Quantum Collision Attacks on Reduced SHA-256 and SHA-512",
                  "abstract": "In this paper, we for the first time show dedicated quantum collision attacks on SHA-256 and SHA-512.\r\nThe attacks reach 38 and 39 steps, respectively, which significantly improve the classical attacks for 31 and 27 steps.\r\nBoth attacks adopt the framework of the previous work that converts many semi-free-start collisions into a 2-block collision, and are faster than the generic attack in the cost metric of time-space tradeoff.\r\nWe observe that the number of required semi-free-start collisions can be reduced in the quantum setting, which allows us to convert the previous classical 38 and 39 step semi-free-start collisions into a collision.\r\nThe idea behind our attacks is simple and will also be applicable to other cryptographic hash functions.",
                  "authors": [
                    "Akinori Hosoyamada",
                    "Yu Sasaki"
                  ],
                  "affiliations": "NTT Corporation and Nagoya University; NTT Corporation",
                  "category": "Uncategorized",
                  "id": "talk-76"
                }
              ],
              "id": "session-14"
            }
          ]
        },
        {
          "starttime": "15:40",
          "endtime": "16:00",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-15"
            }
          ]
        },
        {
          "starttime": "16:00",
          "endtime": "17:00",
          "sessions": [
            {
              "session_title": "A world of SNARKs",
              "id": "session-17",
              "location": {
                "name": "Invited Talk by Jens Groth"
              },
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "17:00",
          "endtime": "17:30",
          "sessions": [
            {
              "id": "session-147",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "17:30",
          "endtime": "18:20",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Succint Arguments",
              "talks": [
                {
                  "paperId": "307",
                  "title": "Halo Infinite: Proof-Carrying Data from Additive Polynomial Commitments",
                  "abstract": "Polynomial commitment schemes (PCS) have recently been in the spotlight for their key role in building SNARKs. A PCS provides the ability to commit to a polynomial over a finite field and prove its evaluation at points. A *succinct* PCS has commitment size and evaluation proof size sublinear in the degree of the polynomial. An *efficient* PCS has sublinear proof verification. Any efficient and succinct PCS can be used to construct a SNARK with similar security and efficiency characteristics (in the random oracle model).\r\n\r\nProof-carrying data (PCD) enables a set of parties to carry out an indefinitely long distributed computation where every step along the way is accompanied by a proof of correctness. It generalizes *incrementally verifiable computation* and can even be used to construct SNARKs. \r\nUntil recently, however, the only known method for constructing PCD required expensive SNARK recursion. A system called *Halo* first demonstrated a new methodology for building PCD without SNARKs, exploiting an aggregation property of the *Bulletproofs* inner-product argument. \r\nThe construction was *heuristic* because it makes non-black-box use of a concrete instantiation of the Fiat-Shamir transform. We expand upon this methodology to show that PCD can be (heuristically) built from any homomorphic polynomial commitment scheme (PCS), even if the PCS evaluation proofs are neither succinct nor efficient. In fact, the Halo methodology extends to any PCS that has an even more general property, namely the ability to aggregate linear combinations of commitments into a new succinct commitment that can later be opened to this linear combination. Our results thus imply new constructions of SNARKs and PCD that were not previously described in the literature and serve as a blueprint for future constructions as well.",
                  "authors": [
                    "Ben Fisch",
                    "Dan Boneh",
                    "Ariel Gabizon",
                    "Justin Drake"
                  ],
                  "affiliations": "Stanford University; Aztec; Ethereum Foundation",
                  "category": "Uncategorized",
                  "id": "talk-101"
                },
                {
                  "paperId": "20",
                  "title": "Proof-Carrying Data without Succinct Arguments",
                  "abstract": "Proof-carrying data (PCD) is a powerful cryptographic primitive that enables mutually distrustful parties to perform distributed computations that run indefinitely. Known approaches to construct PCD are based on succinct non-interactive arguments of knowledge (SNARKs) that have a succinct verifier or a succinct accumulation scheme.\r\n\r\nIn this paper we show how to obtain PCD without relying on SNARKs. We construct a PCD scheme given any non-interactive argument of knowledge (e.g., with linear-size arguments) that has a *split accumulation scheme*, which is a weak form of accumulation that we introduce.\r\n\r\nMoreover, we construct a transparent non-interactive argument of knowledge for R1CS whose split accumulation is verifiable via a (small) *constant number of group and field operations*. Our construction is proved secure in the random oracle model based on the hardness of discrete logarithms, and it leads, via the random oracle heuristic and our result above, to concrete efficiency improvements for PCD.\r\n\r\nAlong the way, we construct a split accumulation scheme for Hadamard products under Pedersen commitments and for a simple polynomial commitment scheme based on Pedersen commitments.\r\n\r\nOur results are supported by a modular and efficient implementation.",
                  "authors": [
                    "Benedikt Bunz",
                    "Alessandro Chiesa",
                    "William Lin",
                    "Pratyush Mishra",
                    "Nick Spooner"
                  ],
                  "affiliations": "Stanford University; UC Berkeley; Boston University",
                  "category": "Uncategorized",
                  "id": "talk-33"
                },
                {
                  "paperId": "61",
                  "title": "Subquadratic SNARGs in the Random Oracle Model",
                  "abstract": "In a seminal work, Micali (FOCS 1994) gave the first succinct non-interactive argument (SNARG) in the random oracle model (ROM). The construction combines a PCP and a cryptographic commitment, and has several attractive features: it is plausibly post-quantum; it can be heuristically instantiated via lightweight cryptography; and it has a transparent (public-coin) parameter setup. However, it also has a significant drawback: a large argument size.\r\n\r\nIn this work, we provide a new construction that achieves a smaller argument size. This is the first progress on the Micali construction since it was introduced over 25 years ago. \r\n\r\nA SNARG in the ROM is (t,ε)-secure if every t-query malicious prover can convince the verifier of a false statement with probability at most ε. For (t,ε)-security, the argument size of all known SNARGs in the ROM (including Micali's) is Õ((log (t/ε))^2) bits, *even* if one were to rely on conjectured probabilistic proofs well beyond current techniques. In practice, these costs lead to SNARGs that are much larger than constructions based on other (pre-quantum and costly) tools. This has led many to believe that SNARGs in the ROM are inherently quadratic.\r\n\r\nWe show that this is not the case. We present a SNARG in the ROM with a sub-quadratic argument size: Õ(log (t/ε) * log t). Our construction relies on a strong soundness notion for PCPs and a weak binding notion for commitments. We hope that our work paves the way for understanding if a linear argument size, that is O(log (t/ε)), is achievable in the ROM.",
                  "authors": [
                    "Alessandro Chiesa",
                    "Eylon Yogev"
                  ],
                  "affiliations": "UC Berkeley; Tel Aviv University",
                  "category": "Uncategorized",
                  "id": "talk-42"
                },
                {
                  "paperId": "33",
                  "title": "Sumcheck Arguments and their Applications",
                  "abstract": "We introduce a class of interactive protocols, which we call *sumcheck arguments*, that establishes a novel connection between the sumcheck protocol (Lund et al. JACM 1992) and folding techniques for Pedersen commitments (Bootle et al. EUROCRYPT 2016).\r\n\r\nInformally, we consider a general notion of bilinear commitment over modules, and show that the sumcheck protocol applied to a certain polynomial associated with the commitment scheme yields a succinct argument of knowledge for openings of the commitment. Building on this, we additionally obtain succinct arguments for the NP-complete language R1CS over certain rings.\r\n\r\nSumcheck arguments enable us to recover as a special case numerous prior works in disparate cryptographic settings (such as discrete logarithms, pairings, RSA groups, lattices), providing one abstract framework to understand them all. Further, we answer open questions raised in prior works, such as obtaining a lattice-based succinct argument from the SIS assumption for satisfiability problems over rings.",
                  "authors": [
                    "Jonathan Bootle",
                    "Alessandro Chiesa",
                    "Katerina Sotiraki"
                  ],
                  "affiliations": "IBM Research Zurich; UC Berkeley",
                  "category": "Uncategorized",
                  "id": "talk-36"
                },
                {
                  "paperId": "403",
                  "title": "An Algebraic Framework for Universal and Updatable SNARKs",
                  "abstract": "We introduce Verifiable Subspace Sampling Arguments, a new information theoretic interactive proof system in which the prover shows that a vector has been sampled in a subspace according to the verifier's coins. We show that this primitive provides a unifying view that explains the technical core of most of the constructions of universal and updatable pairing-based zkSNARKs. This characterization is extended to a fully algebraic framework for designing such zkSNARKs in a modular way, which leads to a new construction that is more efficient than the state-of-the-art in several dimensions.",
                  "authors": [
                    "Arantxa Zapico",
                    "Carla Ràfols"
                  ],
                  "affiliations": "Pompeu Fabra University; Pompeu Fabra University and Cybercat",
                  "category": "Uncategorized",
                  "id": "talk-126"
                }
              ],
              "id": "session-18"
            }
          ]
        },
        {
          "starttime": "18:20",
          "endtime": "19:20",
          "sessions": [
            {
              "id": "session-148",
              "session_title": "Social Break "
            }
          ]
        },
        {
          "starttime": "19:20",
          "endtime": "20:00",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Models",
              "talks": [
                {
                  "paperId": "137",
                  "title": "A Rational Protocol Treatment of 51% Attacks",
                  "abstract": "Game-theoretic analysis of cryptocurrencies and, more generally, blockchain-based decentralized ledgers offers insight on their economic robustness, and their behavior when even the cryptographic assumptions that underpin their security fail. In this work we utilize the recently proposed blockchain adaptation of the rational protocol design (RPD) framework [EUROCRYPT~'18] to analyze 51\\% double-spending attacks against Nakamoto-style cryptocurrencies. We observe a property of the originally proposed utility class that yields an unnatural behavior against such attacks, and show how to devise a utility that avoids this pitfall and makes predictions that match the observable behavior---i.e., that renders attacking a dominant strategy in settings where an attack was indeed observed. We then propose a generic modification to the underlying protocol which deters attacks on consistency by adversaries controlling a majority of the system's resources, including the 51\\% double-spending attack. This can be used as guidance to patch systems that have suffered such attacks, e.g., Ethereum Classic and Bitcoin Cash, and serves as a demonstration of the power of game-theoretic analyses.",
                  "authors": [
                    "Yun Lu",
                    "Vassilis Zikas",
                    "Christian Badertscher"
                  ],
                  "affiliations": "University of Edinburgh; Purdue University; IOHK",
                  "category": "Uncategorized",
                  "id": "talk-60"
                },
                {
                  "paperId": "185",
                  "title": "MoSS: Modular Security Specifications Framework",
                  "abstract": "Applied cryptographic protocols have to meet a rich set of security requirements under diverse environments and against diverse adversaries. However, currently used security specifications, based on either simulation (e.g., `ideal functionality' in UC) or games, are monolithic, combining together different aspects of protocol requirements, environment and assumptions. Such specifications are complex, error-prone, and foil reusability, modular analysis and incremental design. \r\n\r\nWe present the Modular Security Specifications Framework (MoSS) framework, which cleanly separates each security requirement (goal) which a protocol should achieve, from the environment and model (assumptions) under which the requirement should be ensured. This modularity allows us to reuse individual models and requirements across different protocols and tasks, and to compare protocols for the same task, either under different assumptions (models) or satisfying different sets of requirements.  \r\nMoSS is flexible and extendable, e.g., it can support both asymptotic and concrete definitions for security. \r\n\r\nWe demonstrate the applicability of MoSS to two applications: secure broadcast protocols and PKI schemes.",
                  "authors": [
                    "Amir Herzberg",
                    "Hemi Leibowitz",
                    "Ewa Syta",
                    "Sara Wr´otniak"
                  ],
                  "affiliations": "Dept. of Computer Science and Engineering, University of Connecticut, Storrs, CT; Dept. of Computer Science, Bar-Ilan University, Ramat Gan, Israel; Dept. of Computer Science, Trinity College, Hartford, CT",
                  "category": "Uncategorized",
                  "id": "talk-77"
                },
                {
                  "paperId": "320",
                  "title": "Tight State-Restoration Soundness in the Algebraic Group Model",
                  "abstract": "Most efficient zero-knowledge arguments lack a concrete security\r\n  analysis, making parameter choices and efficiency comparisons\r\n  challenging. This is even more true for non-interactive versions of\r\n  these systems obtained via the Fiat-Shamir transform, for which the\r\n  security guarantees generically derived from the interactive\r\n  protocol are often too weak, even when assuming a random oracle.\r\n\r\n  This paper initiates the study of {\\em state-restoration soundness}\r\n  in the algebraic group model (AGM) of Fuchsbauer, Kiltz, and Loss\r\n  (CRYPTO '18). This is a stronger notion of soundness for an\r\n  interactive proof or argument which allows the prover to rewind the\r\n  verifier, and which is tightly connected with the concrete soundness\r\n  of the non-interactive argument obtained via the Fiat-Shamir\r\n  transform.\r\n\r\n  We propose a general methodology to prove tight bounds on\r\n  state-restoration soundness, and apply it to variants of\r\n  Bulletproofs (Bootle et al, S\\&P '18) and Sonic (Maller et al., CCS\r\n  '19). To the best of our knowledge, our analysis of Bulletproofs\r\n  gives the {\\em first} non-trivial concrete security analysis for a\r\n  non-constant round argument combined with the Fiat-Shamir transform.",
                  "authors": [
                    "Ashrujit Ghoshal",
                    "Stefano Tessaro"
                  ],
                  "affiliations": "University of Washington",
                  "category": "Uncategorized",
                  "id": "talk-104"
                },
                {
                  "paperId": "204",
                  "title": "Separating Adaptive Streaming from Oblivious Streaming using the Bounded Storage Model",
                  "abstract": "Streaming algorithms are algorithms for processing large data streams, using only a limited amount of memory. Classical streaming algorithms typically work under the assumption that the input stream is chosen independently from the internal state of the algorithm. Algorithms that utilize this assumption are called oblivious algorithms. Recently, there is a growing interest in studying streaming algorithms that maintain utility also when the input stream is chosen by an adaptive adversary, possibly as a function of previous estimates given by the streaming algorithm. Such streaming algorithms are said to be adversarially-robust. \r\n\r\nBy combining  techniques from learning theory with cryptographic tools from the bounded storage model, we separate the oblivious streaming model from the adversarially-robust streaming model. Specifically, we present a streaming problem for which every adversarially-robust streaming algorithm must use polynomial space, while there exists a classical (oblivious) streaming algorithm that uses only polylogarithmic space. This is the first general separation between the capabilities of these two models, resolving one of the central open questions in adversarial robust streaming.",
                  "authors": [
                    "Haim Kaplan",
                    "Yishay Mansour",
                    "Kobbi Nissim",
                    "Uri Stemmer"
                  ],
                  "affiliations": "Tel Aviv University; Georgetown University; Ben-Gurion University",
                  "category": "Uncategorized",
                  "id": "talk-85"
                }
              ],
              "id": "session-16"
            }
          ]
        },
        {
          "starttime": "20:00",
          "endtime": "20:30",
          "sessions": [
            {
              "id": "session-149",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "20:30",
          "endtime": "21:10",
          "sessions": [
            {
              "id": "session-151",
              "session_title": "Applied Cryptography and Side Channels",
              "talks": [
                {
                  "paperId": "142",
                  "title": "Provable Security Analysis of FIDO2",
                  "abstract": "We carry out the first provable security analysis of the new FIDO2 protocols, the promising FIDO Alliance’s proposal for a standard for passwordless user authentication. Our analysis covers the core components of FIDO2: the W3C’s Web Authentication (WebAuthn) specification and the new Client-to-Authenticator Protocol (CTAP2).\r\n\r\nOur analysis is modular. For WebAuthn and CTAP2, in turn, we propose appropriate security models that aim to capture their intended security goals and use the models to analyze their security. First, our proof confirms the authentication security of WebAuthn. Then, we show CTAP2 can only be proved secure in a weak sense; meanwhile, we identify a series of its design flaws and provide suggestions for improvement. To withstand stronger yet realistic adversaries, we propose a generic protocol called sPACA and prove its strong security; with proper instantiations, sPACA is also more efficient than CTAP2. Finally, we analyze the overall security guarantees provided by FIDO2 and WebAuthn+sPACA based on the security of their components.\r\n\r\nWe expect that our models and provable security results will help clarify the security guarantees of the FIDO2 protocols. In addition, we advocate the adoption of our sPACA protocol as a substitute for CTAP2 for both stronger security and better performance.",
                  "authors": [
                    "Manuel Barbosa",
                    "Alexandra Boldyreva",
                    "Shan Chen",
                    "Bogdan Warinschi"
                  ],
                  "affiliations": "University of Porto (FCUP) and INESC TEC; Georgia Institute of Technology; TU Darmstadt; University of Bristol & Dfinity",
                  "category": "Uncategorized",
                  "id": "talk-61"
                },
                {
                  "paperId": "418",
                  "title": "SSE and SSD: Page-Efficient Searchable Symmetric Encryption",
                  "abstract": "Searchable Symmetric Encryption (SSE) enables a client to outsource a database to an untrusted server, while retaining the ability to securely search the data. The performance bottleneck of classic SSE schemes typically does not come from their fast, symmetric cryptographic operations, but rather from the cost of memory accesses. To address this issue, many works in the literature have considered the notion of locality, a simple design criterion that helps capture the cost of memory accesses in traditional storage media, such as Hard Disk Drives (HDDs). A common thread among many SSE schemes aiming to improve locality is that they are built on top of new memory allocation schemes, which form the technical core of the constructions.\r\nThe starting observation of this work is that for newer storage media such as Solid State Drives (SSDs), which have become increasingly common, locality is not a good predictor of practical performance. Instead, SSD performance mainly depends on page efficiency, that is, reading as few pages as possible. We define this notion, and identify a simple allocation problem, Data-Independent Packing, that captures the main technical challenge required to build page-efficient SSE. As our main result, we build a page-efficient and storage-efficient data-independent packing scheme, and deduce an SSE scheme with the same properties. The technical core of the result relies on a new generalization of cuckoo hashing to items of variable size. Practical experiments show that this approach achieves excellent performance.",
                  "authors": [
                    "Brice Minaud",
                    "Raphael Bost",
                    "Angele Bossuat",
                    "Pierre-Alain Fouque",
                    "Michael Reichle"
                  ],
                  "affiliations": "INRIA and ENS France; DGA; University Rennes",
                  "category": "Uncategorized",
                  "id": "talk-129"
                },
                {
                  "paperId": "191",
                  "title": "Towards Tight Random Probing Security",
                  "abstract": "Proving the security of masked implementations in theoretical models that are relevant to practice and match the best known attacks of the side-channel literature is a notoriously hard problem. The random probing model is a good candidate to contribute to this challenge, due to its ability to capture the continuous nature of physical leakage (contrary to the threshold probing model), while also being convenient to manipulate in proofs and to automate with verification tools. Yet, despite recent progresses in the design of masked circuits with good asymptotic security guarantees in this model, existing results still fall short when it comes to analyze the security of concretely useful circuits under realistic noise levels and with low number of shares. In this paper, we contribute to this issue by introducing a new composability notion, the Probe Distribution Table (PDT), and a new tool (called STRAPS, for the Sampled Testing of the RAndom Probing Security). Their combination allows us to significantly improve the tightness of existing analyses in the most practical (low noise, low number of shares) region of the design space. We illustrate these improvements by quantifying the random probing security of an AES S-box circuit, masked with the popular multiplication gadget of Ishai, Sahai and Wagner from Crypto 2003, with up to six shares.",
                  "authors": [
                    "Gaëtan Cassiers",
                    "Sebastian Faust",
                    "Maximilian Orlt",
                    "François-Xavier Standaert"
                  ],
                  "affiliations": "UCLouvain, Belgium; University of Darmstadt, Germany",
                  "category": "Uncategorized",
                  "id": "talk-80"
                },
                {
                  "paperId": "34",
                  "title": "Secure Wire Shuffling in the Probing Model",
                  "abstract": "In this paper we describe the first improvement of the shuffling countermeasure against side-channel attacks described by Ishai, Sahai and Wagner at Crypto 2003. More precisely, we show how to get worst case statistical security against t probes with running time O(t) instead of O(t log t); our construction is also much simpler. Recall that the classical masking countermeasure achieves perfect security but with running time O(t^2). We also describe a practical implementation for AES that outperforms the masking countermeasure for t ≥ 6 000.",
                  "authors": [
                    "Jean-Sebastien Coron",
                    "Lorenzo Spignoli"
                  ],
                  "affiliations": "University of Luxembourg",
                  "category": "Uncategorized",
                  "id": "talk-37"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "21:10",
          "endtime": "21:30",
          "sessions": [
            {
              "id": "session-152",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "21:30",
          "endtime": "23:30",
          "sessions": [
            {
              "id": "session-153",
              "session_title": "Rump Session",
              "moderator": "Chair: TBD"
            }
          ]
        }
      ]
    },
    {
      "date": "2021-08-19",
      "timeslots": [
        {
          "starttime": "15:00",
          "endtime": "15:40",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Cryptanalysis",
              "talks": [
                {
                  "paperId": "365",
                  "title": "Differential-Linear Cryptanalysis from an Algebraic Perspective",
                  "abstract": "The differential-linear cryptanalysis is an important cryptanalytic tool in cryptography, and has been extensively researched since its discovery by Langford and Hellman in 1994. There are nevertheless very few methods to study the middle part where the differential and linear trail connect, besides the Differential-Linear Connectivity Table (Bar-On et al., EUROCRYPT 2019) and the experimental approach. In this paper, we study differential-linear cryptanalysis from an algebraic perspective. We first introduce a technique called Differential Algebraic Transitional Form (DATF) for differential-linear cryptanalysis, then develop a new theory of estimation of the differential-linear bias and techniques for key recovery in differential-linear cryptanalysis.\r\n\r\nThe techniques are applied to the CAESAR finalist ASCON, the AES finalist SERPENT, and the eSTREAM finalist Grain v1. The bias of the differential-linear approximation is estimated for ASCON and SERPENT. The theoretical estimates of the bias are more accurate than that obtained by the DLCT, and the techniques can be applied with more rounds. Our general techniques can also be used to estimate the bias of Grain v1 in differential cryptanalysis, and have a markedly better performance than the Differential Engine tool tailor-made for the cipher. The improved key recovery attacks on round-reduced variants of these ciphers are then proposed.\r\n\r\nTo the best of our knowledge, they are thus far the best known cryptanalysis of SERPENT, as well as the best differential-linear cryptanalysis of ASCON and the best initialization analysis of Grain v1. The results have been fully verified by experiments. Notably, security analysis of SERPENT is one of the most important applications of differential-linear cryptanalysis in the last two decades. The results in this paper update the differential-linear cryptanalysis of SERPENT-128 and SERPENT-256 with one more round after the work of Biham, Dunkelman and Keller in 2003.",
                  "authors": [
                    "Meicheng Liu",
                    "Xiaojuan Lu",
                    "Dongdai Lin"
                  ],
                  "affiliations": "Chinese Academy of Sciences",
                  "category": "Uncategorized",
                  "id": "talk-115"
                },
                {
                  "paperId": "150",
                  "title": "Meet-in-the-Middle Attacks Revisited: Key-recovery, Collision, and Preimage Attacks",
                  "abstract": "At EUROCRYPT 2021, Bao et al. proposed an automatic method for systematically exploring the configuration space of meet-in-the-middle (MITM) preimage attacks. We further extend it into a constraint-based framework for finding exploitable MITM characteristics in the context of key-recovery and collision attacks by taking the subtle peculiarities of both scenarios into account. Moreover, to perform attacks based on MITM characteristics with nonlinear constrained neutral words, which have not been seen before, we present a procedure for deriving the solution spaces of neutral words without solving the corresponding nonlinear equations or increasing the overall time complexities of the attack. We apply our method to concrete symmetric-key primitives, including SKINNY, ForkSkinny, Romulus, Saturnin, Grostl, and Whirlpool. As a result, we identify the first 23-round key-recovery attack on SKINNY-n-3n and the first 24-round key-recovery attack on ForkSkinny-n-3n in the single-key model with extremely low memories. Moreover, improved (pseudo) preimage or collision attacks on Whirlpool and Grostl are obtained. The source code for generating these attacks is publicly available.",
                  "authors": [
                    "Xiaoyang Dong",
                    "Jialiang Hua",
                    "Siwei Sun",
                    "Zheng Li",
                    "Xiaoyun Wang",
                    "Lei Hu"
                  ],
                  "affiliations": "Institute for Advanced Study, BNRist, Tsinghua University, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China; Beijing University of Technology, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",
                  "category": "Uncategorized",
                  "id": "talk-64"
                },
                {
                  "paperId": "5",
                  "title": "Revisiting the Security of DbHtS MACs: Beyond-Birthday-Bound in the Multi-User Setting",
                  "abstract": "Double-block Hash-then-Sum (\\textsf{DbHtS}) MACs are a class of MACs that aim for achieving beyond-birthday-bound security, including \\textsf{SUM-ECBC}, \\textsf{PMAC\\_Plus}, \\textsf{3kf9} \r\nand \\textsf{LightMAC\\_Plus}. Recently Datta et al. (FSE'19), and then Kim et al. (Eurocrypt'20) prove that \\textsf{DbHtS} constructions are secure beyond the birthday bound in the single-user setting. \r\nHowever, by a generic reduction, their results degrade to (or even worse than) the birthday bound in the multi-user setting. \r\n\r\nIn this work, we revisit the security of \\textsf{DbHtS} MACs in the multi-user setting. We propose a generic framework to prove beyond-birthday-bound security for \\textsf{DbHtS} constructions. \r\nWe demonstrate the usability of this framework with applications to key-reduced variants of \\textsf{DbHtS} MACs, including \\textsf{2k-SUM-ECBC}, \\textsf{2k-PMAC\\_Plus} and \\textsf{2k-LightMAC\\_Plus}. Our results show that the security of these constructions will not degrade as the number of users grows. On the other hand, our results also indicate that these constructions are secure beyond the birthday bound in both single-user and multi-user setting without additional domain separation, which are used in prior works to simplify the analysis.\r\n\r\nMoreover, we find a severe flaw in \\textsf{2kf9}, which is proved to be secure beyond the birthday bound by Datta et al. (FSE'19). We can successfully forge a tag with probability 1 without making any queries. We go further to show attacks with birthday-bound complexity on several variants of \\textsf{2kf9}.",
                  "authors": [
                    "Yaobin Shen",
                    "Lei Wang",
                    "Dawu Gu",
                    "Jian Weng"
                  ],
                  "affiliations": "Shanghai Jiao Tong Univeristy; Shanghai Jiao Tong University; Jinan University",
                  "category": "Uncategorized",
                  "id": "talk-32"
                },
                {
                  "paperId": "323",
                  "title": "Thinking Outside the Superbox",
                  "abstract": "Designing a block cipher or cryptographic permutation can be approached in many different ways. One such approach, popularized by AES, consists in grouping the bits along the S-box boundaries, e.g., in bytes, and in consistently processing them in these groups. This aligned approach leads to hierarchical structures like superboxes that make it possible to reason about the differential and linear propagation properties using combinatorial arguments. In contrast, an unaligned approach avoids any such grouping in the design of transformations. However, without hierarchical structure, sophisticated computer programs are required to investigate the differential and linear propagation properties of the primitive. In this paper, we formalize this notion of alignment and study four primitives that are exponents of different design strategies. We propose a way to analyze the interactions between the linear and the nonlinear layers w.r.t. the differential and linear propagation, and we use it to systematically compare the four primitives using non-trivial computer experiments. We show that alignment naturally leads to different forms of clustering, e.g., of active bits in boxes, of two-round trails in activity patterns, and of trails in differentials and linear approximations.",
                  "authors": [
                    "Daniël Kuijsters",
                    "Gilles Van Assche",
                    "Nicolas Bordes",
                    "Joan Daemen"
                  ],
                  "affiliations": "Radboud University; STMicroelectronics; Université Grenoble Alpes",
                  "category": "Uncategorized",
                  "id": "talk-105"
                }
              ],
              "id": "session-20"
            }
          ]
        },
        {
          "starttime": "15:40",
          "endtime": "15:50",
          "sessions": [
            {
              "session_title": "Mini-Break",
              "id": "session-21"
            }
          ]
        },
        {
          "starttime": "15:50",
          "endtime": "16:20",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Cryptanalysis (continued)",
              "talks": [
                {
                  "paperId": "78",
                  "title": "Cryptanalysis of Full LowMC and LowMC-M with Algebraic Techniques",
                  "abstract": "In this paper, we revisit the difference enumeration techniques for LowMC and develop new algebraic techniques to achieve efficient key-recovery attacks with negligible memory complexity. \\mbox{Benefiting} from our technique to reduce the memory complexity, we could significantly improve the attacks on LowMC when the block size is much larger than the key size and even break LowMC with such a kind of parameter. On the other hand, with our new key-recovery technique, we could significantly improve the time to retrieve the full key if given only a single pair of input and output messages together with the difference trail that they take, which was stated as an interesting question by Rechberger et al. in ToSC 2018. Combining both the techniques, with only 2 chosen plaintexts, we could break 4 rounds of LowMC adopting a full S-Box layer with block size of 129, 192 and 255 bits, respectively, which are the 3 recommended parameters for Picnic3, an alternative \\mbox{third-round} candidate in NIST's Post-Quantum Cryptography competition. We have to emphasize that our attacks do not indicate that Picnic3 is broken as the Picnic use-case is very different and an attacker cannot even freely choose 2 plaintexts to encrypt for a concrete LowMC instance. However, such parameters are deemed as secure in the latest LowMC. Moreover, much more rounds of seven instances of the backdoor cipher \\mbox{LowMC-M} as proposed by Peyrin and Wang in CRYPTO 2020 can be broken without finding the backdoor by making full use of the allowed $2^{64}$ data. The above mentioned attacks are all achieved with negligible memory.",
                  "authors": [
                    "Fukang Liu",
                    "Takanori Isobe",
                    "Willi Meier"
                  ],
                  "affiliations": "East China Normal University; University of Hyogo; University of Hyogo; NICT; PRESTO; FHNW",
                  "category": "Uncategorized",
                  "id": "talk-45"
                },
                {
                  "paperId": "35",
                  "title": "The Cost to Break SIKE: A Comparative Hardware-Based Analysis with AES and SHA-3",
                  "abstract": "This work presents a detailed study of the classical security of the post-quantum supersingular isogeny key encapsulation (SIKE) protocol using a realistic budget-based cost model that considers the actual computing and memory costs that are needed for cryptanalysis. In this effort, we design especially-tailored hardware accelerators for the time-critical multiplication and isogeny computations that we use to model an ASIC-powered instance of the van Oorschot-Wiener (vOW)\r\nparallel collision search algorithm. We then extend the analysis to AES and SHA-3 in the context of the NIST post-quantum cryptography standardization process to carry out a parameter analysis based on our cost model. This analysis, together with the state-of-the-art quantum security\r\nanalysis of SIKE, indicates that the current SIKE parameters offer higher practical security than currently believed, closing an open issue on the suitability of the parameters to match NIST's security levels. In addition, we explore the possibility of using significantly smaller primes to enable\r\nmore efficient and compact implementations with reduced bandwidth. Our improved cost model and analysis can be applied to other cryptographic settings and primitives, and can have implications for other post-quantum candidates in the NIST process.",
                  "authors": [
                    "Patrick Longa",
                    "Wen Wang",
                    "Jakub Szefer"
                  ],
                  "affiliations": "Microsoft; Yale University",
                  "category": "Uncategorized",
                  "id": "talk-38"
                },
                {
                  "paperId": "364",
                  "title": "Improved torsion-point attacks on SIDH variants",
                  "abstract": "SIDH is a post-quantum key exchange algorithm based on the presumed difficulty of finding isogenies between supersingular elliptic curves. However, the exact hardness assumption upon which SIDH relies is not the pure isogeny problem; attackers are also provided with the restriction of the secret isogeny to a subgroup of the curve. Petit [28] leverages this information to break overstretched variants of SIDH in polynomial time, thus demonstrating that exploiting torsion-point information can lead to efficient attacks in some cases. The contribution of this paper is twofold: First, we revisit the techniques of [28] and show how to additionally exploit information coming from a dual and a Frobenius isogeny. We give the full range of parameters to which our attacks apply, an especially interesting instance of which is n-party group key agreement [2]. For 6 or more parties our attack runs in polynomial time and for 3 or more parties our quantum attack improves on the best known asymptotic complexity. We also provide a Magma implementation of our attack for 6 parties. Second, we construct SIDH variants designed to be weak against our attacks; this includes trapdoor choices of starting curve as well as trapdoor choices of base field prime. We stress that our results do not reveal any weakness in the NIST submission SIKE [17].",
                  "authors": [
                    "Victoria de Quehen",
                    "Peter Kutas",
                    "Chris Leonardi",
                    "Chloe Martindale",
                    "Lorenz Panny",
                    "Christophe Petit",
                    "Katherine E. Stange"
                  ],
                  "affiliations": "ISARA Corporation, Waterloo; University of Birmingham; University of Bristol; Academia Sinica; Universite libre de Bruxelles and University of Birmingham; University of Colorado Boulder",
                  "category": "Uncategorized",
                  "id": "talk-114"
                }
              ],
              "id": "session-22"
            }
          ]
        },
        {
          "starttime": "16:20",
          "endtime": "17:20",
          "sessions": [
            {
              "session_title": "Social Break ",
              "id": "session-23"
            }
          ]
        },
        {
          "starttime": "17:20",
          "endtime": "18:10",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Codes and Extractors",
              "talks": [
                {
                  "paperId": "28",
                  "title": "Smoothing Out Binary Linear Codes and Worst-case Sub-exponential Hardness for LPN",
                  "abstract": "Learning parity with noise (LPN) is a notorious (average-case) hard problem that has been well studied in learning theory, coding theory and cryptography since the early 90's. It further inspires the Learning with Errors (LWE) problem [Regev, STOC 2005], which has become one of the central building blocks for post-quantum cryptography and advanced cryptographic. Unlike LWE whose hardness can be reducible from worst-case lattice problems, no corresponding worst-case hardness results were known for LPN until very recently. At Eurocrypt 2019, Brakerski et al. [BLVW19] established the first feasibility result that the worst-case hardness of nearest codeword problem (NCP) (on balanced linear code) at the extremely low noise rate $\\frac{\\log^2 n}{n}$ implies the quasi-polynomial hardness of LPN at the extremely high noise rate $1/2-1/\\poly(n)$. It remained open whether a worst-case to average-case reduction can be established for standard (constant-noise) LPN, ideally with sub-exponential hardness.\r\n\r\nWe start with a simple observation that the hardness of high-noise LPN over large fields is implied by that of the LWE of the same modulus, and is thus reducible from worst-case hardness of lattice problems. We then revisit [BLVW19], which is the main focus of this work. We first expand the underlying binary linear codes (of the NCP) to not only the balanced code considered in [BLVW19] but also to another code (in some sense dual to balanced code). At the core of our reduction is a new variant of smoothing lemma (for both binary codes) that circumvents the barriers (inherent in the underlying worst-case randomness extraction) and admits tradeoffs for a wider spectrum of parameter choices. In addition to the worst-case hardness result obtained in [BLVW19], we show that for any constant $0<c<1$ the constant-noise LPN problem is ($T=2^{\\Omega(n^{1-c})},\\epsilon=2^{-\\Omega(n^{\\min(c,1-c)})},q=2^{\\Omega(n^{\\min(c,1-c)})}$)-hard assuming that the NCP at the low-noise rate $\\tau=n^{-c}$ is ($T'={2^{\\Omega(\\tau n)}}$, $\\epsilon'={2^{-\\Omega(\\tau n)}}$,$m={2^{\\Omega(\\tau n)}}$)-hard in the worst case, where $T$, $\\epsilon$, $q$ and $m$ are time complexity, success rate, sample complexity, and codeword length respectively. Moreover, refuting the worst-case hardness assumption would imply arbitrary polynomial speedups over the current state-of-the-art algorithms for solving the NCP (and LPN), which is a win-win result. Unfortunately, public-key encryptions and collision resistant hash functions need constant-noise LPN with ($T={2^{\\omega(\\sqrt{n})}}$, $\\epsilon'={2^{-\\omega(\\sqrt{n})}}$,$q={2^{\\sqrt{n}}}$)-hardness (Yu et al., CRYPTO 2016 \\& ASIACRYPT 2019), which is almost (up to an arbitrary $\\omega(1)$ factor in the exponent) what is reducible from the worst-case NCP when $c= 0.5$. We leave it as an open problem whether the gap can be closed or there is a separation in place.",
                  "authors": [
                    "Yu Yu",
                    "Jiang Zhang"
                  ],
                  "affiliations": "Shanghai Jiao Tong University, Shanghai Qizhi Institute, and Shanghai Key Laboratory of Privacy-Preserving Computation; State Key Laboratory of Cryptology, China",
                  "category": "Uncategorized",
                  "id": "talk-34"
                },
                {
                  "paperId": "280",
                  "title": "Silver: Silent VOLE and Oblivious Transfer from Hardness of Decoding Structured LDPC Codes",
                  "abstract": "We put forth new protocols for oblivious transfer extension and vector OLE, called \\emph{Silver}, for SILent Vole and oblivious transfER. Silver offers extremely high performances: generating 10 million random OTs on one core of a standard laptop requires only 300ms of computation and 122KB of communication. This represents 37% less computation and ~1300x less communication than the standard IKNP protocol, as well as ~4x less computation and ~4x less communication than the recent protocol of Yang et al. (CCS 2020). Silver is \\emph{silent}: after a one-time cheap interaction, two parties can store small seeds, from which they can later \\emph{locally} generate a large number of OTs \\emph{while remaining offline}. Neither IKNP nor Yang et al. enjoys this feature; compared to the best known silent OT extension protocol of Boyle et al. (CCS 2019), upon which we build up, Silver has 19x less computation, and the same communication. Due to its attractive efficiency features, Silver yields major efficiency improvements in numerous MPC protocols.\r\n    \r\n    Our approach is a radical departure from the standard paradigm for building MPC protocols, in that we do \\emph{not} attempt to base our constructions on a well-studied assumption. Rather, we follow an approach closer in spirit to the standard paradigm in the design of symmetric primitives: we identify a set of fundamental structural properties that allow us to withstand all known attacks, and put forth a candidate design, guided by our analysis. We also rely on extensive experimentations to analyze our candidate and experimentally validate their properties. In essence, our approach boils down to constructing new families of linear codes with (plausibly) high minimum distance and extremely low encoding time. While further analysis is of course warranted to confidently assess the security of Silver, we hope and believe that initiating this approach to the design of MPC primitives will pave the way to new secure primitives with extremely attractive efficiency features.",
                  "authors": [
                    "Peter Rindal",
                    "Srinivasan Raghuraman",
                    "Geoffroy Couteau"
                  ],
                  "affiliations": "Visa Research; CNRS, IRIF, Université de Paris",
                  "category": "Uncategorized",
                  "id": "talk-96"
                },
                {
                  "paperId": "180",
                  "title": "Non-Malleable Codes for Bounded Parallel-Time Tampering",
                  "abstract": "Non-malleable codes allow one to encode data in such a way that once a codeword is being tampered with, the modified codeword is either an encoding of the original message, or a completely unrelated one. Since the introduction of this notion by Dziembowski, Pietrzak, and Wichs (ICS '10 and J. ACM '18), there has been a large body of works realizing such coding schemes secure against various classes of tampering functions. It is well known that there is no efficient non-malleable code secure against all polynomial size tampering functions. Nevertheless,  no code which is non-malleable for \\emph{bounded} polynomial size attackers is known and obtaining such a code has been a major open problem.\r\n\r\nWe present the first construction of a non-malleable code secure against  all polynomial size tampering functions that have {bounded} parallel time. This is an even larger class than all bounded polynomial size functions. In particular, this class includes all functions in non-uniform $\\mathbf{NC}$ (and  much more). Our construction is in the plain model (i.e., no trusted setup) and relies on several cryptographic assumptions such as keyless hash functions, time-lock puzzles, as well as other standard assumptions. Additionally, our construction has several appealing properties: the complexity of encoding is independent of the class of tampering functions and we can obtain (sub-)exponentially small error.",
                  "authors": [
                    "Dana Dachman-Soled",
                    "Ilan Komargodski",
                    "Rafael Pass"
                  ],
                  "affiliations": "University of Maryland; Hebrew University and NTT Research; Cornell Tech",
                  "category": "Uncategorized",
                  "id": "talk-75"
                },
                {
                  "paperId": "299",
                  "title": "Improved Computational Extractors and their Applications",
                  "abstract": "Recent exciting breakthroughs have achieved the first two-source extractors that operate in the low min-entropy regime. Unfortunately, these constructions suffer from non-negligible error, and reducing the error to negligible remains an important open problem. In recent work, Garg, Kalai, and Khurana (GKK, Eurocrypt 2020) investigated a meaningful relaxation of this problem to the computational setting, in the presence of a common random string (CRS). In this relaxed model, their work built explicit two-source extractors for a restricted class of unbalanced sources with min-entropy n^{\\gamma} (for some constant \\gamma) and negligible error, under the sub-exponential DDH assumption.\r\n\r\nIn this work, we investigate whether computational extractors in the CRS model be applied to more challenging environments. Specifically, we study network extractor protocols (Kalai et al., FOCS 2008) and extractors for adversarial sources (Chattopadhyay et al., STOC 2020) in the CRS model. We observe that these settings require extractors that work well for balanced sources, making the GKK results inapplicable. \r\n\r\nWe remedy this situation by obtaining the following results, all of which are in the CRS model and assume the sub-exponential hardness of DDH.\r\n\r\n- We obtain ``optimal'' computational two-source and non-malleable extractors for balanced sources: requiring both sources to have only poly-logarithmic min-entropy, and achieving negligible error. To obtain this result, we perform a tighter and arguably simpler analysis of the GKK extractor.\r\n    \r\n- We obtain a single-round network extractor protocol for poly-logarithmic min-entropy sources that tolerates an optimal number of adversarial corruptions. Prior work in the information-theoretic setting required sources with high min-entropy rates, and in the computational setting had round complexity that grew with the number of parties, required sources with linear min-entropy, and relied on exponential hardness (albeit without a CRS).\r\n    \r\n- We obtain an ``optimal'' adversarial source extractor for poly-logarithmic min-entropy sources, where the number of honest sources is only 2 and each corrupted source can depend on either one of the honest sources. Prior work in the information-theoretic setting had to assume a large number of honest sources.",
                  "authors": [
                    "Dakshita Khurana",
                    "Akshayaram Srinivasan"
                  ],
                  "affiliations": "UIUC; Tata Institute of Fundamental Research",
                  "category": "Uncategorized",
                  "id": "talk-98"
                },
                {
                  "paperId": "279",
                  "title": "Adaptive Extractors and their Application to Leakage Resilient Secret Sharing",
                  "abstract": "We introduce Adaptive  Extractors, which unlike traditional randomness extractors, guarantee security even when an adversary obtains leakage on the source \\textit{after} observing the extractor output. We make a compelling case for the study of such extractors by demonstrating their use in obtaining adaptive leakage in secret sharing schemes. \r\n\r\nSpecifically, at FOCS 2020, Chattopadhyay, Goodman, Goyal, Kumar, Li, Meka, Zuckerman, built an adaptively secure leakage resilient secret sharing scheme (LRSS) with both rate and leakage rate being $\\mathcal{O}(1/n)$, where $n$ is the number of parties. In this work, we build an adaptively secure LRSS that offers an interesting trade-off between rate, leakage rate, and the total number of shares from which an adversary can obtain leakage. As a special case, when considering $t$-out-of-$n$ secret sharing schemes for threshold $t = \\alpha n$ (constant $0<\\alpha<1$), we build a scheme with constant rate, constant leakage rate, and allow the adversary leakage from all but $t-1$ of the shares, while giving her the remaining $t-1$ shares completely in the clear. (Prior to this, constant rate LRSS scheme tolerating adaptive leakage was unknown for \\textit{any} threshold.)\r\n \r\nFinally, we show applications of our techniques to both non-malleable secret sharing and secure message transmission.",
                  "authors": [
                    "Nishanth Chandran",
                    "Bhavana Kanukurthi",
                    "Sai Lakshmi Bhavana Obbattu",
                    "Sruthi Sekar"
                  ],
                  "affiliations": "Microsoft Research, India; Indian Institute of Science, Bangalore",
                  "category": "Uncategorized",
                  "id": "talk-95"
                }
              ],
              "id": "session-24"
            }
          ]
        },
        {
          "starttime": "18:10",
          "endtime": "18:40",
          "sessions": [
            {
              "id": "session-154",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "18:40",
          "endtime": "19:10",
          "sessions": [
            {
              "id": "session-150",
              "session_title": "Secret Sharing",
              "talks": [
                {
                  "paperId": "164",
                  "title": "Upslices, Downslices, and Secret-Sharing with Complexity of $1.5^n$",
                  "abstract": "A secret-sharing scheme allows to distribute a secret $s$ among $n$ parties such that only some predefined ``authorized'' sets of parties can reconstruct the secret, and all other ``unauthorized'' sets learn nothing about $s$.  The collection of authorized/unauthorized sets is be captured by a monotone function $f:\\{0,1\\}^n\\rightarrow \\{0,1\\}$. In this paper, we focus on monotone functions that all their min-terms are sets of size $a$, and on their duals -- monotone functions whose max-terms are of size $b$. We refer to these classes as $(a,n)$-\\emph{upslices} and $(b,n)$-\\emph{downslices}, and note that these natural families correspond to monotone $a$-regular DNFs and monotone $(n-b)$-regular CNFs. We derive the following results.\r\n\r\n\\begin{enumerate}\r\n    \\item (General downslices) Every downslice can be realized with total share size of $1.5^{n+o(n)}<2^{0.585 n}$. Since every monotone function can be cheaply decomposed into $n$ downslices, we obtain a similar result for general access structures improving the previously known $2^{0.637n+o(n)}$ complexity of Applebaum, Beimel, Nir and Peter (STOC 2020). We also achieve a minor improvement in the exponent of linear secrets sharing schemes. \r\n\r\n    \\item (Random mixture of upslices) Following, Beimel and Farr{\\`{a}}s (TCC 2020) who studied the complexity of random DNFs with constant-size terms, we consider the following general distribution $F$ over monotone DNFs: For each width value $a\\in [n]$, uniformly sample $k_a$ monotone terms of size $a$, where $\\vec{k}=(k_1,\\ldots,k_n)$ is an arbitrary vector of non-negative integers. We show that, except with exponentially small probability, $F$ can be realized with share size of $2^{0.5 n+o(n)}$ and can be linearly realized with an exponent strictly smaller than $2/3$. Our proof also provides a candidate distribution for the ``exponentially-hard'' access structure. \r\n\\end{enumerate}\r\n\r\nWe use our results to explore connections between several seemingly unrelated questions about the complexity of secret-sharing schemes such as worst-case vs. average-case, linear vs. non-linear, and primal vs. dual access structures. We prove that, in at least one of these settings, there is a significant gap in secret-sharing complexity.",
                  "authors": [
                    "Oded Nir",
                    "Benny Applebaum"
                  ],
                  "affiliations": "Tel Aviv University",
                  "category": "Uncategorized",
                  "id": "talk-69"
                },
                {
                  "paperId": "44",
                  "title": "Asymptotically-Good Arithmetic Secret Sharing over Z/p^{\\ell}Z with Strong Multiplication and Its Applications to Efficient MPC",
                  "abstract": "The current paper studies information-theoretically secure multiparty computation (MPC) over rings $\\Z/p^{\\ell}\\Z$. This is a follow-up research of recent work on MPC over rings $\\Z/p^{\\ell}\\Z$. In the work of \\cite[TCC2019]{tcc}, a protocol based on the Shamir secret sharing over $\\Z/p^{\\ell}\\Z$ was presented. As in the field case, its limitation is that the share size has to grow as the number of players increases. Then several MPC protocols were developed in \\cite[Asiacrypt 2020]{asiacrypt} to overcome this limitation. However, the MPC protocols in \\cite[Asiacrypt 2020]{asiacrypt} suffer from several drawbacks: (i) the offline multiplication gate has super-linear communication complexity;\r\n(ii) the share size is doubled for the most important case, namely over $\\Z/2^{\\ell}\\Z$ due to infeasible lifting of self-orthogonal codes from fields to rings; (iii) most importantly, the BGW model could not be applied via the secret sharing given in \\cite[Asiacrypt 2020]{asiacrypt} due to lack of strong multiplication.\r\n\r\nOur contribution in this paper is three fold. Firstly, we overcome all the drawbacks in \\cite{tcc,asiacrypt} mentioned above. Secondly, we establish an arithmetic secret sharing with strong multiplication, which is the most important primitive in the BGW model. Thirdly, we lift Reverse Multiplication Friendly Embeddings (RMFE) from fields to rings, with same (linear) complexity. Note that RMFE has become a standard technique for amortized communication complexity in MPC, as in \\cite[CRYPTO'18]{crypto2018} and \\cite[CRYPTO'19]{dn19}.\r\n\r\nTo obtain our theoretical results, we use the existence of lifts of curves over rings, then use the known results stating that Riemann-Roch spaces are free modules. To make our scheme practical, we start from good algebraic geometry codes over finite fields obtained from existing computational techniques. Then we present, and implement, an efficient algorithm to Hensel-lift the generating matrix of the code, such that the multiplicative conditions are preserved over rings. Existence of this specific lift is guaranteed by the previous theory. On the other hand, a random lifting of codes over from fields to Galois rings does not preserve multiplicativity in general. (Notice that our indirect method is motivated by the fact that, following the theory instead, would require to ``preprocess'' the curve under a form with ``smooth\" equations, in particular with many variables, before lifting it. But computing on these objects over rings is out of the scope of existing research). Finally we provide efficient elementary methods for sharing and (robust) reconstruction of secrets over rings. As a result, arithmetic secret sharing over $\\Z/p^{\\ell}\\Z$ with strong multiplication can be efficiently constructed and practically applied.",
                  "authors": [
                    "Ronald Cramer",
                    "Matthieu Rambaud",
                    "Chaoping Xing"
                  ],
                  "affiliations": "CWI Amsterdam & Leiden University; Telecom Paris, Institut polytechnique de Paris; Shanghai Jiao Tong University",
                  "category": "Uncategorized",
                  "id": "talk-40"
                },
                {
                  "paperId": "388",
                  "title": "Large Message Homomorphic Secret Sharing from DCR and Applications",
                  "abstract": "We present the first homomorphic secret sharing (HSS) construction that simultaneously (1) has negligible correctness error, (2) supports integers from an exponentially large range, and (3) relies on an assumption not known to imply FHE --- specifically, the Decisional Composite Residuosity (DCR) assumption. This resolves an open question posed by Boyle, Gilboa, and Ishai (Crypto 2016). Homomorphic secret sharing is analogous to fully-homomorphic encryption, except the ciphertexts are shared across two non-colluding evaluators. Previous constructions of HSS either had non-negligible correctness error and polynomial-size plaintext space or were based on the stronger LWE assumption. We also present two applications of our technique: a multi-server ORAM with constant bandwidth overhead, and a rate-$1$ trapdoor hash function with negligible error rate.",
                  "authors": [
                    "Jaspal Singh",
                    "Lawrence Roy"
                  ],
                  "affiliations": "Oregon State University",
                  "category": "Uncategorized",
                  "id": "talk-122"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "19:10",
          "endtime": "19:20",
          "sessions": [
            {
              "id": "session-155",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "19:20",
          "endtime": "19:50",
          "sessions": [
            {
              "id": "session-156",
              "session_title": "Secret Sharing (continued)",
              "talks": [
                {
                  "paperId": "196",
                  "title": "Traceable Secret Sharing and Applications",
                  "abstract": "Consider a scenario where Alice stores some secret data $s$ on $n$ servers using a $t$-out-of-$n$ secret sharing scheme. Trudy (the collector) is interested in the secret data of Alice and is willing to pay for it. Trudy publishes an advertisement on the internet which describes an elaborate cryptographic scheme to collect the shares from the $n$ servers. Each server who decides to submit its share is paid a hefty monetary reward and is guaranteed ``immunity\" from being caught or prosecuted in a court for violating its service agreement with Alice. Bob is one of the servers and sees this advertisement. On examining the collection scheme closely, Bob concludes that there is no way for Alice to prove anything in a court that he submitted his share. Indeed, if Bob is rational, he might use the cryptographic scheme in the advertisement and submit his share since there are no penalties and no fear of being caught and prosecuted. Can we design a secret sharing scheme which Alice can use to avoid such a scenario?\r\n\r\nWe introduce a new primitive called as \\textit{Traceable Secret Sharing} to tackle this problem. In particular, a traceable secret sharing scheme guarantees that a cheating server always runs the risk of getting traced and prosecuted by providing a valid evidence (which can be examined in a court of law) implicating its dishonest behavior. We explore various definitional aspects and show how they are highly non-trivial to construct (even ignoring efficiency aspects). We then give an efficient construction of traceable secret sharing assuming the existence of a secure two-party computation protocol. We also show an application of this primitive in constructing traceable protocols for multi-server delegation of computation.",
                  "authors": [
                    "Vipul Goyal",
                    "Yifan Song",
                    "Akshayaram Srinivasan"
                  ],
                  "affiliations": "CMU and NTT Research; CMU; Tata Institute of Fundamental Research",
                  "category": "Uncategorized",
                  "id": "talk-82"
                },
                {
                  "paperId": "351",
                  "title": "Quadratic Secret Sharing and Conditional Disclosure of Secrets",
                  "abstract": "There is a huge gap between the upper and lower bounds on the share size of secret-sharing schemes for arbitrary $n$-party access structures, and consistent with our current knowledge the optimal share size can be anywhere between polynomial in $n$ and exponential in $n$. For linear secret-sharing schemes, we know that the share size for almost all $n$-party access structures must be exponential in $n$. Furthermore, most constructions of efficient secret-sharing schemes are linear. We would like to study larger classes of secret-sharing schemes  with two goals.  On one hand, we want to prove lower bounds for larger classes of secret-sharing schemes, possibly shedding some light on the share size of general secret-sharing schemes. On the other hand, we want to construct efficient secret-sharing schemes for access structures that do not have efficient linear secret-sharing schemes. Given this motivation, Paskin-Cherniavsky and Radune (ITC'20) defined and studied a new class of secret-sharing schemes in which the shares are generated by applying  (low-degree) polynomials to the secret and some random field elements. The special case $d=1$ corresponds to linear and multi-linear secret sharing schemes. \r\n\r\nWe define and study two additional classes of polynomial secret-sharing schemes: (1) schemes in which for every authorized set the reconstruction of the secret is done using polynomials and (2) schemes in which both sharing and reconstruction are done by polynomials. For linear secret-sharing schemes, schemes with linear sharing and schemes with linear reconstruction are  equivalent.  We give evidence that for polynomial secret-sharing schemes, schemes with polynomial sharing are probably stronger than schemes with polynomial reconstruction. We also prove lower bounds on the share size for schemes with polynomial reconstruction. On the positive side, we provide constructions of secret-sharing schemes and conditional disclosure of secrets (CDS) protocols with polynomials of degree-$2$ sharing and reconstruction.   \r\nWe extend a construction of Liu et al. (CRYPTO'17) and construct a degree-$2$ $k$-server CDS protocols for a function $f:[N]^k\\rightarrow \\set{0,1}$ with message size $O(N^{(k-1)/3})$. We also show how to transform our degree-$2$ $k$-server CDS protocol to a robust CDS protocol, and use the robust CDS protocol \r\nto construct degree-$2$ secret-sharing schemes for arbitrary access structures with share size $O(2^{0.716n})$;\r\nthis is better than  the best known share size of $O(2^{0.762n})$ for linear secret-sharing schemes and worse than the best known share size of $O(2^{0.637n})$ for general secret-sharing schemes.",
                  "authors": [
                    "Amos Beimel",
                    "Hussien Othman",
                    "Naty Peter"
                  ],
                  "affiliations": "Ben-Gurion University of the Negev; Tel-Aviv University",
                  "category": "Uncategorized",
                  "id": "talk-111"
                },
                {
                  "paperId": "361",
                  "title": "Constructing Locally Leakage-resilient Linear Secret-sharing Schemes",
                  "abstract": "Innovative side-channel attacks have repeatedly falsified the assumption that cryptographic implementations are opaque black-boxes. Therefore, it is essential to ensure cryptographic constructions' security even when information leaks via unforeseen avenues. One such fundamental cryptographic primitive is the secret-sharing schemes, which underlies nearly all threshold cryptography. Our understanding of the leakage-resilience of secret-sharing schemes is still in its preliminary stage. \r\n\r\nThis work studies locally leakage-resilient linear secret-sharing schemes. An adversary can leak $m$ bits of arbitrary local leakage from each $n$ secret shares. However, in a locally leakage-resilient secret-sharing scheme, the leakage's joint distribution reveals no additional information about the secret. \r\n\r\nFor every constant $m$, we prove that the Massey secret-sharing scheme corresponding to a random linear code of dimension $k$ (over sufficiently large prime fields) is locally leakage-resilient, where $k/n > 1/2$ is a constant. The previous best construction by Benhamouda, Degwekar, Ishai, Rabin (CRYPTO--2018) needed $k/n > 0.907$. A technical challenge arises because the number of all possible $m$-bit local leakage functions is exponentially larger than the number of random linear codes. Our technical innovation begins with identifying an appropriate pseudorandomness-inspired family of tests; passing them suffices to ensure leakage-resilience. We show that most linear codes pass all tests in this family. This Monte-Carlo construction of linear secret-sharing scheme that is locally leakage-resilient has applications to leakage-resilient secure computation. \r\n\r\nDrawing inspiration from the technical approach above, we prove the leakage-resilience of Shamir's secret-sharing scheme for $k\\geq 0.8675 n$. Furthermore, we highlight a crucial bottleneck for all the analytical approaches in this line of work. Benhamouda et al. introduced an analytical proxy to study the leakage-resilience of secret-sharing schemes; if the proxy is small, then the scheme is leakage-resilient. However, we present a one-bit local leakage function demonstrating that the converse is false, motivating the need for new analytically well-behaved functions that capture leakage-resilience more accurately. \r\n\r\nTechnically, the analysis involves probabilistic and combinatorial techniques and (discrete) Fourier analysis. The family of new ``tests'' capturing local leakage functions, we believe, is of independent and broader interest.",
                  "authors": [
                    "Hemanta K. Maji",
                    "Anat Paskin-Cherniavsky",
                    "Tom Suad",
                    "Mingyuan Wang"
                  ],
                  "affiliations": "Purdue University; Ariel University",
                  "category": "Uncategorized",
                  "id": "talk-113"
                }
              ]
            }
          ]
        },
        {
          "starttime": "19:50",
          "endtime": "20:20",
          "sessions": [
            {
              "id": "session-157",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "20:20",
          "endtime": "21:10",
          "sessions": [
            {
              "id": "session-158",
              "session_title": "Zero-Knowledge",
              "talks": [
                {
                  "paperId": "148",
                  "title": "Witness Authenticating NIZKs and Applications",
                  "abstract": "We initiate the study of witness authenticating NIZK  proof systems (WA-NIZKs), in which one can use a witness $w$ of a statement $x$ to identify whether a valid proof for $x$ is indeed generated using $w$.  Such a new identification functionality also puts new requirements on soundness that (1) no adversary can generate a valid proof that will not be identified by any witness; (2) or forge a proof using her valid witness to frame others. \r\n\r\nTo work around the obvious obstacle towards conventional zero-knowledgeness, we define entropic zero-knowledgeness that requires the proof to leak no partial information, if the witness is unpredictable, or more precisely, has sufficient computational entropy. \r\n\t\r\nWe give a formal treatment of this new primitive. The modeling turns out to be quite involved and multiple subtle  points arise and particular cares are required. We present general constructions from standard assumptions. We also demonstrate three applications in non-malleable (perfect one-way) hash, group signatures with verifier-local revocations and plaintext-checkable public-key encryption. Our WA-NIZK provides a new tool to advance the state of the art in all these applications.",
                  "authors": [
                    "Hanwen Feng",
                    "Qiang Tang"
                  ],
                  "affiliations": "Beihang University; University of Sydney",
                  "category": "Uncategorized",
                  "id": "talk-63"
                },
                {
                  "paperId": "422",
                  "title": "Towards a Unified Approach to Black-Box Constructions of Zero-Knowledge Proofs",
                  "abstract": "General-purpose zero-knowledge proofs for all NP languages greatly simplify secure protocol design. However, they inherently require the code of the underlying relation. If the relation contains black-box calls to a cryptographic function, the code of that function must be known in order to use the ZK proof, even if both the relation as well as the proof require only black-box access to the function. Rosulek (CRYPTO'12) shows that non-trivial proofs for even simple statements, such as membership in the range of a one-way function, require non-black-box access.\r\n\r\nWe propose an alternative approach to bypass Rosulek's impossibility result. Instead of asking for a ZK proof directly for the given one-way function f, we seek to construct a new one-way function F given only black-box access to f, and an associated ZK protocol for proving non-trivial statements, such as range membership, over its output. We say that F, along with its proof system, is a proof-based one-way function. We similarly define proof-based versions of other primitives, specifically, pseudorandom generators and collision-resistant hash functions.\r\n\r\nWe show how to construct proof-based versions of each of the aforementioned primitives from their ordinary counterparts under mild restrictions over the input. More specifically,\r\n\\begin{itemize}\r\n\\item \r\nWe first show that if the input is entirely chosen by the prover, then proof-based pseudorandom generators cannot be constructed from ordinary ones in a black-box manner, thus establishing that some restrictions over the input are necessary.\r\n\\item \r\nWe next present black-box constructions handling inputs of the form  (x,r) where r is chosen uniformly by the verifier. This is similar to the restrictions in the widely used Goldreich-Levin theorem. The associated ZK proofs support range-membership over the output as well as arbitrary predicates over prefixes of the input.\r\n\\end{itemize}\r\n\r\nOur results open up the possibility that general-purpose ZK proofs for relations that require black-access to the aforementioned primitives may be possible in future without violating their black-box nature, by instantiating them using proof-based primitives instead of ordinary ones.",
                  "authors": [
                    "Xiao Liang",
                    "Omkant Pandey"
                  ],
                  "affiliations": "Stony Brook University",
                  "category": "Uncategorized",
                  "id": "talk-131"
                },
                {
                  "paperId": "151",
                  "title": "Compressing Proofs of k-Out-Of-n Partial Knowledge",
                  "abstract": "In an (honest-verifier) zero-knowledge proof of partial knowledge, introduced by Cramer, Damg{\\aa}rd and Schoenmakers (CRYPTO 1994), a prover knowing witnesses for some $k$-subset of $n$ given public statements can convince the verifier of this claim without revealing which $k$-subset.\r\nThe accompanying solution cleverly combines $\\Sigma$-protocol theory and linear secret sharing, and achieves linear communication complexity for general $k,n$.\r\nEspecially the ``one-out-of-$n$'' case $k=1$ has seen myriad applications during the last decades, e.g., in electronic voting, ring signatures, and confidential transaction systems in general.\r\n\r\nIn this paper we focus on the discrete logarithm (DL) setting, where the prover claims knowledge of DLs of $k$-out-of-$n$ given elements. \r\nGroth and  Kohlweiss (EUROCRYPT 2015) have shown how to solve the special case $k=1$ %, yet arbitrary~$n$, \r\nwith {\\em logarithmic} (in $n$) communication, instead of linear as prior work. However, their method %, which is original, \r\ntakes explicit advantage of $k=1$ and does not generalize to $k>1$ without losing all advantage over prior work.\r\nAlternatively, an {\\em indirect} approach for solving the considered problem is by translating the $k$-out-of-$n$ relation into a circuit and then applying recent advances in communication-efficient circuit ZK. Indeed, for the $k=1$ case this approach has been highly optimized, e.g., in ZCash.\r\n\r\nOur main contribution is a new, simple honest-verifier zero-knowledge proof protocol for proving knowledge of $k$ out of $n$ DLs with {\\em logarithmic} communication and {\\em for general $k$ and $n$}, without requiring any generic circuit ZK machinery.\r\nOur solution  %deploys a novel twist on \r\nputs forward a novel extension of the\r\n{\\em compressed} $\\Sigma$-protocol theory (CRYPTO 2020), which we then utilize to compress a %carefully chosen adaptation of the CRYPTO 1994 approach \r\nnew $\\Sigma$-protocol for proving knowledge of $k$-out-of-$n$ DL's down to logarithmic size. The latter $\\Sigma$-protocol is inspired by the CRYPTO 1994 approach, but a careful re-design of the original protocol is necessary for the compression technique to apply. \r\nInterestingly, {\\em even for $k=1$ and general $n$} our approach improves prior {\\em direct} approaches as it reduces prover complexity without increasing the communication complexity.\r\nBesides the conceptual simplicity, \r\nwe also identify regimes of  \r\npractical relevance where our approach achieves asymptotic and concrete improvements, e.g., in proof size and prover complexity, over the generic approach based on circuit-ZK. \r\n\r\nFinally, we show various extensions and generalizations of our core result. For instance, we extend our protocol to proofs of partial knowledge of Pedersen (vector) commitment openings, and/or to include a proof that the witness satisfies some additional constraint, and we show how to extend our results to non-threshold access structures.",
                  "authors": [
                    "Thomas Attema",
                    "Ronald Cramer",
                    "Serge Fehr"
                  ],
                  "affiliations": "CWI & TNO; CWI & Leiden University",
                  "category": "Uncategorized",
                  "id": "talk-65"
                },
                {
                  "paperId": "348",
                  "title": "Mac'n'Cheese: Zero-Knowledge Proofs for Boolean and Arithmetic Circuits with Nested Disjunctions",
                  "abstract": "Zero knowledge proofs are an important building block in many cryptographic applications.\r\n    Unfortunately, when the proof statements become very large, existing\r\n    zero-knowledge proof systems easily reach their limits: either the computational\r\n    overhead, the memory footprint, or the required bandwidth exceed levels that\r\n    would be tolerable in practice.\r\n\r\n    We present an interactive zero-knowledge proof system for boolean and\r\n    arithmetic circuits, called Mac'n'Cheese, with a focus on supporting large\r\n    circuits. Our work follows the commit-and-prove paradigm instantiated using\r\n    information-theoretic MACs based on vector oblivious linear evaluation to\r\n    achieve high efficiency. We additionally show how to optimize disjunctions,\r\n    with a general OR transformation for proving the disjunction of $m$\r\n    statements that has communication complexity proportional to the longest\r\n    statement (plus an additive term logarithmic in $m$). These disjunctions can\r\n    further be \\emph{nested}, allowing efficient proofs about complex statements\r\n    with many levels of disjunctions. We also show how to make Mac'n'Cheese\r\n    non-interactive (after a preprocessing phase) using the Fiat-Shamir\r\n    transform, and with only a small degradation in soundness.\r\n\r\n    We have implemented the online phase of Mac'n'Cheese and achieve a runtime of 144~ns per AND\r\n    gate and 1.5~$\\mu$s per multiplication gate in $\\mathbb{F}_{2^{61}-1}$ when run over a network\r\n    with a 95~ms latency and a bandwidth of 31.5~Mbps. In addition, we show that\r\n    the disjunction optimization improves communication as expected: when\r\n    proving a boolean circuit with eight branches and each branch containing\r\n    roughly 1 billion multiplications, Mac'n'Cheese requires only 75 more bytes to\r\n    communicate than in the single branch case.",
                  "authors": [
                    "Carsten Baum",
                    "Alex J. Malozemoff",
                    "Marc B. Rosen",
                    "Peter Scholl"
                  ],
                  "affiliations": "Aarhus University; Galois, Inc.",
                  "category": "Uncategorized",
                  "id": "talk-110"
                },
                {
                  "paperId": "416",
                  "title": "Time- and Space-Efficient Arguments from Groups of Unknown Order",
                  "abstract": "We construct public-coin time- and space-efficient zero-knowledge arguments for NP. For every time T and space S non-deterministic RAM computation, the prover runs in time T * polylog(T) and space S * polylog(T), and the verifier runs in time n * polylog(T), where n is the input length. Our protocol relies on hidden order groups, which can be instantiated without a trusted setup using class groups, and can heuristically be made non-interactive using the Fiat-Shamir transform.\r\n\r\nOur proof builds on DARK (Bunz et al., Eurocrypt 2020), a recent succinct and efficiently verifiable polynomial commitment scheme. We show how to implement a variant of DARK in a time- and space-efficient way. Along the way we:\r\n\r\n1. Identify a significant gap in the proof of security of Dark.\r\n\r\n2. Give a non-trivial modification of the DARK scheme that overcomes the aforementioned gap. The modified version also relies on significantly weaker cryptographic assumptions than those in the original DARK scheme. Our proof utilizes ideas from the theory of integer lattices in a novel way.\r\n\r\n3. Generalize Pietrzak's (ITCS 2019) proof of exponentiation (PoE) protocol to work with general groups of unknown order (without relying on any cryptographic assumption).\r\n\r\nIn proving these results, we develop general-purpose techniques for working with (hidden order) groups, which may be of independent interest.",
                  "authors": [
                    "Alexander Block",
                    "Justin Holmgren",
                    "Alon Rosen",
                    "Ron D. Rothblum",
                    "Pratik Soni"
                  ],
                  "affiliations": "Purdue University; NTT Research; IDC Herzliya; Technion; Carnegie Mellon University",
                  "category": "Uncategorized",
                  "id": "talk-128"
                }
              ]
            }
          ]
        },
        {
          "starttime": "21:10",
          "endtime": "21:30",
          "sessions": [
            {
              "id": "session-159",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "21:30",
          "endtime": "22:30",
          "sessions": [
            {
              "id": "session-160",
              "session_title": "Membership Meeting"
            }
          ]
        }
      ]
    },
    {
      "date": "2021-08-20",
      "timeslots": [
        {
          "starttime": "15:00",
          "endtime": "15:50",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Multi-Party Computation 3",
              "talks": [
                {
                  "paperId": "30",
                  "title": "Pushing the Limits of Valiant's Universal Circuits: Simpler, Tighter and More Compact",
                  "abstract": "A universal circuit (UC) is a general-purpose circuit that can simulate arbitrary circuits (up to a certain size $n$). Valiant provides a $k$-way recursive construction of UCs (STOC 1976), where $k$ tunes the complexity of the recursion. More concretely, Valiant gives theoretical constructions of 2-way and 4-way UCs of asymptotic (multiplicative) sizes $5n\\log n$ and $4.75 n\\log n$ respectively, which matches the asymptotic lower bound $\\Omega(n\\log n)$ up to some constant factor. \r\n\r\nMotivated by various privacy-preserving cryptographic applications, Kiss et al. (Eurocrypt 2016) validated the practicality of $2$-way universal circuits by giving example implementations for private function evaluation. G{\\\"{u}}nther et al. (Asiacrypt 2017) and Alhassan et al. (J. Cryptology 2020) implemented the 2-way/4-way hybrid UCs with various optimizations in place towards making universal circuits more practical. Zhao et al. (Asiacrypt 2019) optimized Valiant's 4-way UC to asymptotic size $4.5 n\\log n$ and proved a lower bound $3.64 n\\log n$ for UCs under the Valiant framework.\r\n\r\nAs the scale of computation goes beyond 10-million-gate ($n=10^7$) or even billion-gate level ($n=10^9$), the constant factor in UCs size plays an increasingly important role in application performance. In this work, we investigate Valiant's universal circuits and present an improved framework for constructing universal circuits with the following advantages.\r\n\r\n[Simplicity.] Parameterization is no longer needed. In contrast to that previous implementations resorted to a hybrid construction combining $k=2$ and $k=4$ for a tradeoff between fine granularity and asymptotic size-efficiency, our construction gets the best of both worlds when configured at the lowest complexity (i.e., $k=2$).\r\n\r\n[Compactness.] Our universal circuits have asymptotic size $3n\\log n$, improving upon the best previously known $4.5n\\log n$ by 33\\% and beating the $3.64n\\log n$ lower bound for UCs constructed under Valiant's framework (Zhao et al., Asiacrypt 2019).\r\n\r\n[Tightness.] We show that under our new framework the UCs size is lower bounded by $2.95 n\\log n$, which almost matches the $3n\\log n$ circuit size of our $2$-way construction.\r\n\r\nWe implement the 2-way universal circuits and evaluate its performance with other implementations, which confirms our theoretical analysis.",
                  "authors": [
                    "Hanlin Liu",
                    "Yu Yu",
                    "Shuoyao Zhao",
                    "Jiang Zhang",
                    "Wenling Liu",
                    "Zhenkai Hu"
                  ],
                  "affiliations": "Shanghai Jiao Tong University; Shanghai Jiao Tong University, Shanghai Qizhi Institute, and Shanghai Key Laboratory of Privacy-Preserving Computation; State Key Laboratory of Cryptology, China",
                  "category": "Uncategorized",
                  "id": "talk-35"
                },
                {
                  "paperId": "253",
                  "title": "Oblivious Key-Value Stores and Amplification for Private Set Intersection",
                  "abstract": "Many recent private set intersection (PSI) protocols encode input sets as polynomials. We consider the more general notion of an oblivious key-value store (OKVS), which is a data structure that compactly represents a desired mapping $k_i$ to $v_i$. When the $v_i$ values are random, the OKVS data structure hides the $k_i$ values that were used to generate it. The simplest (and size-optimal) OKVS is a polynomial $p$ that is chosen using interpolation such that $p(k_i)=v_i$.\r\n\r\nWe initiate the formal study of oblivious key-value stores, and show new constructions resulting in the fastest OKVS to date.\r\n\r\nSimilarly to cuckoo hashing, current analysis techniques are insufficient for finding *concrete* parameters to guarantee a small failure probability for our OKVS constructions. Moreover,\r\nit would cost too much to run experiments to validate  a small upperbound on the failure probability. We therefore show novel techniques to amplify an  OKVS construction which has a failure probability $p$, to an OKVS with a similar overhead and failure probability $p^c$. Setting $p$ to be moderately small enables to  validate it by running a relatively small number of $O(1/p)$ experiments. This validates a $p^c$ failure probability for the amplified OKVS. \r\n\t\t\r\nFinally, we describe how OKVS can significantly improve the state of the art of essentially all variants of PSI. This leads to the fastest two-party PSI protocols to date, for both the semi-honest and the malicious settings. Specifically, in networks with moderate bandwidth (e.g., 30 - 300 Mbps) our malicious  two-party PSI  protocol has 40\\% less communication and is 20-40% faster than the previous state of the art protocol, even though the latter only has heuristic confidence.",
                  "authors": [
                    "Gayathri Garimella",
                    "Benny Pinkas",
                    "Mike Rosulek",
                    "Ni Trieu",
                    "Avishay Yanai"
                  ],
                  "affiliations": "Oregon State University; Bar-Ilan University; Arizona State University; VMware",
                  "category": "Uncategorized",
                  "id": "talk-92"
                },
                {
                  "paperId": "378",
                  "title": "MHz2k: MPC from HE over $\\mathbb{Z}_{2^k}$ with New Packing, Simpler Reshare, and Better ZKP",
                  "abstract": "We propose a multiparty computation protocol over $\\mathbb{Z}_{2^k}$ secure against actively corrupted majority from somewhat homomorphic encryption. The main technical contributions are: (i) a new efficient packing method for messages in $\\mathbb{Z}_{2^k}$ for lattice-based somewhat homomorphic encryption schemes, (ii) a simpler reshare protocol for level-dependent packings, (iii) a more efficient zero-knowledge proof of plaintext knowledge on a cyclotomic ring $\\Z[X]/\\Phi_M(X)$ with $M$ being a prime. Integrating them, our protocol shows from 2.2x upto 4.9x improvements in amortized communication costs compared to the previous best results. Our techniques not only improve the efficiency of MPC over the $\\mathbb{Z}_{2^k}$ considerably, but also elucidate several properties of $\\mathbb{Z}_{2^k}$ which can be exploited when designing other cryptographic primitives over $\\mathbb{Z}_{2^k}$.",
                  "authors": [
                    "Jung Hee Cheon",
                    "Dongwoo Kim",
                    "Keewoo Lee"
                  ],
                  "affiliations": "Seoul National University; Western Digital Research, Milpitas",
                  "category": "Uncategorized",
                  "id": "talk-120"
                },
                {
                  "paperId": "393",
                  "title": "Sublinear GMW-Style Compiler for MPC with Preprocessing",
                  "abstract": "We consider the efficiency of protocols for secure multiparty computation (MPC) with a dishonest majority. A popular approach for the design of such protocols is to employ {\\em preprocessing}. Before the inputs are known, the parties generate correlated secret randomness, which is consumed by a fast and ``information-theoretic'' online protocol. \r\n\r\nA powerful technique for securing such protocols against malicious parties uses {\\em homomorphic MACs} to authenticate the values produced by the online protocol. Compared to a baseline protocol, which is only secure against semi-honest parties, this involves a significant increase in the size of the correlated randomness, by a factor of up to a statistical security parameter. Different approaches for partially mitigating this extra storage cost come at the expense of increasing the online communication.\r\n\r\nIn this work we propose a new technique for protecting MPC with preprocessing against malicious parties. We show that for circuit evaluation protocols that satisfy mild security and structural requirements, that are met by almost all standard protocols with semi-honest security, the extra {\\em additive} storage and online communication costs are both {\\em logarithmic} in the circuit size. This applies to Boolean circuits and to arithmetic circuits over fields or rings, and to both information-theoretic and computationally secure protocols. Our protocol can be viewed as a sublinear information-theoretic variant of the celebrated ``GMW compiler'' that applies to MPC with preprocessing.\r\n\r\nOur compiler makes a novel use of the techniques of Boneh et al. (Crypto 2019) for sublinear distributed zero knowledge, which were previously only used in the setting of {\\em honest-majority} MPC.",
                  "authors": [
                    "Ariel Nof",
                    "Elette Boyle",
                    "Yuval Ishai",
                    "Niv Gilboa"
                  ],
                  "affiliations": "Technion Israel Institute of Technology; IDC Herzliya; Ben-Gurion University",
                  "category": "Uncategorized",
                  "id": "talk-123"
                },
                {
                  "paperId": "401",
                  "title": "Limits on the Adaptive Security of Yao’s Garbling",
                  "abstract": "Yao’s garbling scheme is one of the most fundamental cryptographic constructions. Lindell and Pinkas (Journal of Cryptograhy 2009) gave a formal proof of security in the selective setting assuming secure symmetric-key encryption (and hence one-way functions). This was fol- lowed by results, both positive and negative, concerning its security in the, stronger, adaptive setting. Applebaum et al. (Crypto 2013) showed that it cannot satisfy adaptive security as is, due to a simple incompressibility argument. Jafagholi and Wichs (TCC 2017) considered a natural adaptation of Yao’s scheme that circumvents this negative result, and proved that it is adaptively secure, at least for shallow circuits. In particular, they showed that for the class of circuits of depth d, the loss in security is at most exponential in d. The above results all concern the simulation-based notion of security.\r\n\r\nIn this work, we show that the upper bound of Jafargholi and Wichs is more or less optimal in a strong sense. As our main result, we show that there exists a family of Boolean circuits, one for each depth d ∈ N, such that any black-box reduction proving the adaptive indistinguishability- security of the natural adaptation of Yao’s scheme from any symmetric-key encryption has to lose a factor that is sub-exponential in d. Since indistinguishability is a weaker notion than simulation, our bound also applies to adaptive simulation.\r\n\r\nTo establish our results, we build on the recent approach of Kamath et al. (Eprint 2021), which uses pebbling lower bounds in conjunction with oracle separations to prove fine-grained lower bounds on loss in cryptographic security",
                  "authors": [
                    "Chethan Kamath",
                    "Karen Klein",
                    "Krzysztof Pietrzak",
                    "Daniel Wichs"
                  ],
                  "affiliations": "Unaffilated; IST Austria; Northeastern University",
                  "category": "Uncategorized",
                  "id": "talk-125"
                }
              ],
              "id": "session-26"
            }
          ]
        },
        {
          "starttime": "15:50",
          "endtime": "16:20",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-27"
            }
          ]
        },
        {
          "starttime": "16:20",
          "endtime": "17:10",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Encryption++",
              "talks": [
                {
                  "paperId": "117",
                  "title": "Broadcast Encryption with Size N^{1/3} and More from k-Lin",
                  "abstract": "We present the first pairing-based ciphertext-policy attribute-based encryption (CP-ABE) scheme for the class of degree 3 polynomials with compact parameters: the public key, ciphertext and secret keys comprise O(n) group elements, where n is input length for the function. As an immediate corollary, we obtain a pairing-based broadcast encryption scheme for N users with O(N^1/3)- sized parameters, giving the first significant parameter improvements in pairing- based broadcast encryption in over a decade. All of our constructions achieve adaptive security against unbounded collusions, and rely on the (bilateral) k-Lin assumption in prime-order bilinear groups.",
                  "authors": [
                    "Hoeteck Wee"
                  ],
                  "affiliations": "NTT Research and ENS, Paris",
                  "category": "Uncategorized",
                  "id": "talk-56"
                },
                {
                  "paperId": "49",
                  "title": "Fine-grained Secure Attribute-based Encryption",
                  "abstract": "Fine-grained cryptography is constructing cryptosystems in a setting where an adversary’s resource is a-prior bounded and an honest party has less resource than an adversary. Currently, only simple form of encryption schemes, such as secret-key and public-key encryption, are constructed in this setting.\r\nIn this paper, we enrich the available tools in fine-grained cryptography by proposing the first fine-grained secure attribute-based encryption (ABE) scheme. Our construction is adaptively secure under the widely accepted worst-case assumption, $NC1 \\subsetneq \\oplus L/poly$, and it is presented in a generic manner using the notion of predicate encodings (Wee, TCC’14). By properly instantiating the underlying encoding, we can obtain different types of ABE schemes, including identity-based encryption. Previously, all of these schemes were unknown in fine-grained cryptography. Our main technical contribution is constructing ABE schemes without using pairing or the Diffie-Hellman assumption. Hence, our results show that, even if one-way functions do not exist, we still have ABE schemes with meaningful security. For more application of our techniques, we construct an efficient (quasi-adaptive) non-interactive zero-knowledge (QA-NIZK) proof system.",
                  "authors": [
                    "Yuyu Wang",
                    "Jiaxin Pan",
                    "Yu Chen"
                  ],
                  "affiliations": "University of Electronic Science and Technology of China; NTNU – Norwegian University of Science and Technology; Shandong University",
                  "category": "Uncategorized",
                  "id": "talk-41"
                },
                {
                  "paperId": "171",
                  "title": "Multi-Input Quadratic Functional Encryption from Pairings",
                  "abstract": "We construct the first multi-input functional encryption (MIFE) scheme for quadratic functions from pairings. Our construction supports polynomial number of users, where user $i$, for $i \\in [n]$, encrypts input $\\bfx_i \\in \\mbZ^m$ to obtain ciphertext $\\ct_i$, the key generator provides a key $\\sk_\\bfc$ for vector $\\bfc \\in \\mbZ^{({mn})^2}$ and decryption, given $\\ct_1,\\ldots,\\ct_n$ and $\\sk_\\bfc$, recovers $\\ip{\\bfc}{\\bfx \\otimes \\bfx}$ and nothing else. We achieve  indistinguishability-based (selective) security against unbounded collusions under the standard bilateral matrix Diffie-Hellman assumption. All previous MIFE schemes either support only inner products (linear functions) or rely on strong cryptographic assumptions such as indistinguishability obfuscation or multi-linear maps.",
                  "authors": [
                    "Shweta Agrawal",
                    "Rishab Goyal",
                    "Junichi Tomida"
                  ],
                  "affiliations": "IIT Madras; MIT; NTT Secure Platform Labs",
                  "category": "Uncategorized",
                  "id": "talk-73"
                },
                {
                  "paperId": "271",
                  "title": "Functional Encryption for Turing Machines with Dynamic Bounded Collusion from LWE",
                  "abstract": "The classic work of Gorbunov, Vaikuntanathan and Wee (CRYPTO 2012) and follow-ups provided constructions of bounded collusion Functional Encryption (FE) for circuits from mild assumptions. In this work, we improve the state of affairs for bounded collusion FE in several ways:\r\n\r\n\\begin{enumerate}\r\n\\item {\\it New Security Notion.} We introduce the notion of  {\\it dynamic} bounded collusion FE, where the declaration of collusion bound is delayed to the time of encryption. This enables the encryptor to dynamically choose the collusion bound for different ciphertexts depending on their individual level of sensitivity. Hence, the ciphertext size grows linearly with its own collusion bound and the public key size is independent of collusion bound. In contrast, all prior constructions have public key and ciphertext size that grow at least linearly with a fixed bound $Q$.\r\n\r\n\\item {\\it CPFE for circuits with Dynamic Bounded Collusion.} We provide the first CPFE schemes for circuits enjoying dynamic bounded collusion security. \r\nBy assuming identity based encryption (IBE), we construct CPFE for circuits of {\\it unbounded} size satisfying {\\it non adaptive} simulation based security. By strengthening the underlying assumption to IBE with receiver selective opening security, we obtain CPFE for circuits of {\\it bounded} size, output length and depth enjoying {\\it adaptive} simulation based security. Moreover, we show that IBE is a necessary assumption for these primitives.  \\\\\r\nMoreover, by relying on the Learning With Errors (LWE) assumption, we obtain the first {\\it succinct} CPFE for circuits, i.e. supporting circuits with unbounded size, but fixed output length and depth. This scheme achieves {\\it adaptive} simulation based security. \r\n\r\n\\item {\\it KPFE for circuits with dynamic bounded collusion.} We provide the first KPFE for circuits of unbounded size, but bounded depth and output length satisfying dynamic bounded collusion security. Our construction relies on LWE and achieves {\\it adaptive} simulation based security. This improves the security of succinct KPFE by Goldwasser et al. \\cite{GKPVZ13a}. \r\n\r\n\\item {\\it KP and CP FE for TM/NL with dynamic bounded collusion.} We provide the first KPFE and CPFE constructions of bounded collusion functional encryption for Turing machines in the public key setting from LWE. Our constructions achieve non-adaptive simulation based security. Both the input and the machine in our construction can be of {\\it unbounded} polynomial length but the ciphertext size grows with the upper bound on the running time of the Turing machine on the given input. Given RAM access to the ciphertext, the scheme enjoys input specific decryption time.\r\n\r\nWe provide a variant of the above scheme that satisfies {\\it adaptive} security, but at the cost of supporting a smaller class of computation, namely Nondeterministic Logarithmic-space (NL). Since NL contains Nondeterministic Finite Automata (NFA), this result subsumes {\\it all} prior work of bounded collusion FE for uniform models from standard assumptions \\cite{AMY19,AS17}. \r\n\r\n\\end{enumerate}",
                  "authors": [
                    "Shweta Agrawal",
                    "Shota Yamada",
                    "Monosij Maitra",
                    "Narasimha Sai Vempati"
                  ],
                  "affiliations": "IIT Madras; AIST Japan; TU Darmstadt",
                  "category": "Uncategorized",
                  "id": "talk-94"
                },
                {
                  "paperId": "160",
                  "title": "Receiver-Anonymity in Reradomizable RCCA-Secure Cryptosystems Resolved",
                  "abstract": "In this work, we resolve the open problem raised by Prabhakaran and Rosulek at CRYPTO 2007, and present the first anonymous, rerandomizable, Replayable-CCA (RCCA) secure public key encryption scheme. This solution opens the door to numerous privacy-oriented applications with a highly desired RCCA security level. At the core of our construction is a non-trivial extension of smooth projective hash functions (Cramer and Shoup, EUROCRYPT 2002), and a modular generic framework developed for constructing Rand-RCCA-secure encryption schemes with receiver-anonymity. The framework gives an enhanced abstraction of the original Prabhakaran and Rosulek’s scheme (which was the first construction of Rand-RCCA-secure encryption in the standard model), where the most crucial enhancement is the first realization of the desirable property of receiver-anonymity, essential to privacy settings. It also serves as a conceptually more intuitive and generic understanding of RCCA security, which leads, for example, to new implementations of the notion. Finally, note that (since CCA security is not applicable to the privacy applications motivating our work) the concrete results and the conceptual advancement presented here, seem to substantially expand the power and relevance of the notion of Rand-RCCA-secure encryption.",
                  "authors": [
                    "Yi Wang",
                    "Rongmao Chen",
                    "Guomin Yang",
                    "Xinyi Huang",
                    "Baosheng Wang",
                    "Moti Yung"
                  ],
                  "affiliations": "National University of Defense Technology; University of Wollongong; Fujian Normal University; Columbia University",
                  "category": "Uncategorized",
                  "id": "talk-68"
                }
              ],
              "id": "session-28"
            }
          ]
        },
        {
          "starttime": "17:10",
          "endtime": "18:10",
          "sessions": [
            {
              "session_title": "Social Break ",
              "id": "session-29"
            }
          ]
        },
        {
          "starttime": "18:10",
          "endtime": "18:40",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Foundations",
              "talks": [
                {
                  "paperId": "1",
                  "title": "White Box Traitor Tracing",
                  "abstract": "Traitor tracing aims to identify the source of leaked decryption keys. Since the ``traitor'' can try to hide their key within obfuscated code in order to evade tracing, the tracing algorithm should work for general, potentially obfuscated, decoder \\emph{programs}. In the setting of such general decoder programs, prior work uses \\emph{black box} tracing: the tracing algorithm ignores the implementation of the decoder, and instead traces just by making queries to the decoder and observing the outputs.\r\n\t\r\nWe observe that, in some settings, such black box tracing leads to consistency and user privacy issues. On the other hand, these issues do not appear inherent to \\emph{white box} tracing, where the tracing algorithm actually inspects the decoder implementation. We therefore develop new white box traitor tracing schemes providing consistency and/or privacy. Our schemes can be instantiated under various assumptions ranging from public key encryption to indistinguishability obfuscation, with different trade-offs. To the best of our knowledge, ours is the first work to consider white box tracing in the general decoder setting.",
                  "authors": [
                    "Mark Zhandry"
                  ],
                  "affiliations": "Princeton University & NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-31"
                },
                {
                  "paperId": "103",
                  "title": "Does Fiat-Shamir Require a Cryptographic Hash Function?",
                  "abstract": "The Fiat-Shamir transform is a general method for reducing interaction in public-coin protocols by replacing the random verifier messages with deterministic hashes of the protocol transcript. The soundness of this transformation is usually heuristic and lacks a formal security proof. Instead, to argue security, one can rely on the random oracle methodology, which informally states that whenever a random oracle soundly instantiates Fiat-Shamir, a hash function that is ``sufficiently unstructured'' (such as fixed-length SHA-2) should suffice. Finally, for some special interactive protocols, it is known how to (1) isolate a concrete security property of a hash function that suffices to instantiate Fiat-Shamir and (2) build a hash function satisfying this property under a cryptographic assumption such as Learning with Errors.\r\n\r\nIn this work, we abandon this methodology and ask whether Fiat-Shamir truly requires a cryptographic hash function. Perhaps surprisingly, we show that in two of its most common applications --- building signature schemes as well as (general-purpose) non-interactive zero-knowledge arguments --- there are sound Fiat-Shamir instantiations using extremely simple and non-cryptographic hash functions such as sum-mod-$p$ or bit decomposition. In some cases, we make idealized assumptions (i.e., we invoke the generic group model), while in others, we prove soundness in the plain model.\r\n\r\nOn the negative side, we also identify important cases in which a cryptographic hash function is provably necessary to instantiate Fiat-Shamir. We hope this work leads to an improved understanding of the precise role of the hash function in the Fiat-Shamir transformation.",
                  "authors": [
                    "Yilei Chen",
                    "Alex Lombardi",
                    "Fermi Ma",
                    "Willy Quach"
                  ],
                  "affiliations": "Tsinghua University; MIT; Princeton and NTT Research; Northeastern",
                  "category": "Uncategorized",
                  "id": "talk-53"
                },
                {
                  "paperId": "241",
                  "title": "Composition with Knowledge Assumptions",
                  "abstract": "Zero-knowledge succinct non-interactive arguments (zk-SNARKs) rely on knowledge assumptions for their security. Meanwhile, as the complexity and scale of cryptographic systems continues to grow, the composition of secure protocols is of vital importance. The current gold standards of composable security, the Universal Composability and Constructive Cryptography frameworks cannot capture knowledge assumptions, as their core proofs of composition prohibit white-box extraction. In this paper, we present a formal model allowing the composition of knowledge assumptions. Despite showing impossibility for the general case, we demonstrate the model’s usefulness when limiting knowledge assumptions to few instances of protocols at a time. We finish by providing the first instance of a simultaneously succinct and composable zk-SNARK, by using existing results within our framework.",
                  "authors": [
                    "Thomas Kerber",
                    "Aggelos Kiayias",
                    "Markulf Kohlweiss"
                  ],
                  "affiliations": "The University of Edinburgh & IOHK",
                  "category": "Uncategorized",
                  "id": "talk-91"
                }
              ],
              "id": "session-30"
            }
          ]
        },
        {
          "starttime": "18:40",
          "endtime": "18:50",
          "sessions": [
            {
              "id": "session-161",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "18:50",
          "endtime": "19:20",
          "sessions": [
            {
              "id": "session-162",
              "session_title": "Foundations (continued)",
              "talks": [
                {
                  "paperId": "312",
                  "title": "Non-Interactive Batch Arguments for NP from Standard Assumptions",
                  "abstract": "We study the problem of designing *non-interactive batch arguments* for NP. Such an argument system allows an efficient prover to prove multiple $\\npol$ statements, with size much smaller than the combined witness length. \r\n\r\nWe provide the first construction of such an argument system for NP in the common reference string model based on standard cryptographic assumptions. Prior works either require non-falsifiable assumptions (or the random oracle model) or can only support private verification.\r\n\r\n\r\nAt the heart of our result is a new *dual mode* interactive batch argument system for NP. We show how to apply the correlation-intractability framework for Fiat-Shamir -- that has primarily been applied to proof systems -- to such interactive arguments.",
                  "authors": [
                    "Arka Rai Choudhuri",
                    "Abhishek Jain",
                    "Zhengzhong Jin"
                  ],
                  "affiliations": "Johns Hopkins University",
                  "category": "Uncategorized",
                  "id": "talk-102"
                },
                {
                  "paperId": "339",
                  "title": "Targeted Lossy Functions and Applications",
                  "abstract": "Lossy trapdoor functions, introduced by Peikert and Waters (STOC '08), can be initialized in one of two indistinguishable modes: in injective mode, the function preserves all information about its input, and can be efficiently inverted given a trapdoor, while in lossy mode, the function loses some information about its input. Such functions have found countless applications in cryptography, and can be constructed from a  variety of Cryptomania assumptions.  In this work, we introduce \\emph{targeted lossy functions (TLFs)}, which relax lossy trapdoor functions along two orthogonal dimensions. Firstly,  they do not require an inversion trapdoor in injective mode. Secondly, the lossy mode of the function is initialized with some target input, and the function is only required to lose information about this particular target. The injective and lossy modes should be indistinguishable even given the target.  We  construct TLFs from Minicrypt assumptions, namely, injective pseudorandom generators, or even one-way functions under a natural relaxation of injectivity.  We then generalize TLFs to  incorporate \\emph{branches}, and construct \\emph{all-injective-but-one} and \\emph{all-lossy-but-one} variants. We show a wide variety of applications of targeted lossy functions. In several cases, we get the first Minicrypt constructions of primitives that were previously only known under Cryptomania assumptions.  Our applications include:\r\n        \\begin{itemize}\r\n        \t\\item  \\emph{Pseudo-entropy functions}\r\n                  from one-way functions.\r\n        \t\\item Deterministic leakage-resilient message-authentication codes and improved leakage-resilient symmetric-key encryption from one-way functions. \r\n        \t\\item Extractors for \\emph{extractor-dependent sources} \r\n        \t       from one-way functions. \r\n        \t\\item Selective-opening secure symmetric-key encryption from one-way functions. \r\n        \t\\item A new construction of CCA PKE from (exponentially secure) trapdoor functions and injective pseudorandom generators. \r\n        \\end{itemize}\r\n        We also discuss a fascinating connection to distributed point functions.",
                  "authors": [
                    "Willy Quach",
                    "Brent Waters",
                    "Daniel Wichs"
                  ],
                  "affiliations": "Northeastern University; University of Texas at Austin and NTT Research",
                  "category": "Uncategorized",
                  "id": "talk-107"
                },
                {
                  "paperId": "375",
                  "title": "The $t$-wise Independence of Substitution-Permutation Networks",
                  "abstract": "This paper promotes and continues a research program aimed at {\\em proving} the security of block ciphers such as AES against important and well-studied classes of attacks. In particular, we initiate the study of (almost) $t$-wise independence of concrete block-cipher  construction paradigms such as substitution-permutation networks and key-alternating ciphers. This is a meaningful target, as sufficiently strong (almost) pairwise independence already suffices to resist (truncated) differential attacks and linear cryptanalysis. Our results are two-fold.\r\n\r\n\\begin{enumerate}\r\n\\item \r\nOur first result concerns substitution-permutation networks (SPNs) that model block ciphers such as AES. We prove the {\\em almost pairwise-independence} of an SPN instantiated with a concrete S-box such as the patched inverse function $x \\mapsto x^{-1}$ as well as an appropriate linear mixing layer, given sufficiently many rounds and independent sub-keys. Our proof relies on a {\\em characterization} of S-box computation on input differences in terms of sampling output differences from certain sub-spaces, and a new randomness extraction lemma (which we prove with Fourier-analytic techniques) that establishes when such sampling yields uniformity. \r\n\r\n\\item \r\nSecondly, we show that  instantiating a key-alternating cipher (which can be thought of as a degenerate case of SPNs) with most permutations gives us (almost) $t$-wise independence in $t + o(t)$ rounds. In order to do this, we use the probabilistic method to develop two new lemmas, an independence-amplification lemma and a distance amplification lemma, that allow us to reason about the evolution of key-alternating ciphers. \r\n\r\n\\end{enumerate}",
                  "authors": [
                    "Tianren Liu",
                    "Stefano Tessaro",
                    "Vinod Vaikuntanathan"
                  ],
                  "affiliations": "University of Washington; MIT",
                  "category": "Uncategorized",
                  "id": "talk-117"
                }
              ]
            }
          ]
        },
        {
          "starttime": "19:20",
          "endtime": "19:30",
          "sessions": [
            {
              "id": "session-163",
              "session_title": "Mini-Break"
            }
          ]
        },
        {
          "starttime": "19:30",
          "endtime": "20:00",
          "sessions": [
            {
              "id": "session-164",
              "session_title": "Low-Complexity Cryptography",
              "talks": [
                {
                  "paperId": "370",
                  "title": "Low-Complexity Weak Pseudorandom Functions in AC0[MOD2]",
                  "abstract": "A *weak pseudorandom function* (WPRF) is a keyed function $f_k:\\{0,1\\}^n\\to\\{0,1\\}$ such that, for a random key $k$, a collection of samples $(x, f_k(x))$, for {\\em uniformly random} inputs $x$, cannot be efficiently distinguished from totally random input-output pairs $(x,y)$. We study WPRFs in AC0[MOD2], the class of functions computable by AC0 circuits with parity gates, making the following contributions. \r\n\r\n- *Between Lapland and Cryptomania.* We show that WPRFs in AC0[MOD2] imply a variant of the Learning Parity with Noise (LPN) assumption. This gives an unconditional version of an earlier conditional result of Akavia et al. (ITCS 2014). We further show that WPRFs in a subclass of AC0[mod 2] that includes a recent WPRF candidate by Boyle et al. (FOCS 2020) imply, under a seemingly weak additional conjecture, public-key encryption. \r\n\r\n- *WPRF by sparse polynomials.* We propose the first WPRF candidate that can be computed by sparse multivariate polynomials over $\\F_2$. We prove that it has subexponential security against linear and algebraic attacks.\r\n\r\n- *WPRF in AC0 ◦ MOD2.* We study the existence of WPRFs computed by AC0 circuits \\emph{over} parity gates. We propose a modified version of a previous WPRF candidate of Akavia et al., and prove that it resists the algebraic attacks that were used by Bogdanov and Rosen (ECCC 2017) to break the original candidate in quasipolynomial time. We give evidence against the possibility of using {\\em public} parity gates and relate this question to other conjectures.",
                  "authors": [
                    "Elette Boyle",
                    "Geoffroy Couteau",
                    "Niv Gilboa",
                    "Yuval Ishai",
                    "Lisa Kohl",
                    "Peter Scholl"
                  ],
                  "affiliations": "IDC Herzliya; CNRS, IRIF, Université de Paris; Ben-Gurion University; Technion; CWI Amsterdam; Aarhus University",
                  "category": "Uncategorized",
                  "id": "talk-116"
                },
                {
                  "paperId": "377",
                  "title": "MPC-Friendly Symmetric Cryptography from Alternating Moduli: Candidates, Protocols, and Applications",
                  "abstract": "We study new candidates for symmetric cryptographic primitives that leverage alternation between linear functions over $\\Z_2$ and $\\Z_3$ to support fast protocols for secure multiparty computation (MPC). This continues the study of weak pseudorandom functions of this kind initiated by Boneh et al. (TCC 2018) and Cheon et al. (ePrint 2020).\r\n\r\nWe make the following contributions. \r\nCandidates. We propose new designs of symmetric primitives based on alternating moduli. These include candidate one-way functions, pseudorandom generators, and weak pseudorandom functions in which the key, the  input and the output are all over $\\Z_2$. We propose concrete parameters based on cryptanalysis. \r\n\r\nProtocols. We provide a unified approach for securely evaluating modulus-alternating primitives in different MPC models. For the original candidate of Boneh et al., our protocols obtain roughly 2x improvement in all performance measures. We report efficiency benchmarks of an optimized implementation.\r\n\r\nApplications. We showcase the usefulness of our candidates for a variety of applications. This includes short \"Picnic-style\" signature schemes, as well as protocols for oblivious pseudorandom functions, hierarchical key derivation, and distributed key generation for function secret sharing.",
                  "authors": [
                    "Itai Dinur",
                    "Steven Goldfeder",
                    "Tzipora Halevi",
                    "Yuval Ishai",
                    "Mahimna Kelkar",
                    "Vivek Sharma",
                    "Greg Zaverucha"
                  ],
                  "affiliations": "Ben-Gurion University; Cornell Tech; Brooklyn College, CUNY; Technion; Cornell Tech, Cornell University; Graduate Center, CUNY; Microsoft",
                  "category": "Uncategorized",
                  "id": "talk-119"
                },
                {
                  "paperId": "41",
                  "title": "No Time to Hash: On Super-Efficient Entropy Accumulation",
                  "abstract": "Real-world random number generators (RNGs) cannot afford to use (slow) cryptographic hashing every time they refresh their state R with a new entropic input X. Instead, they use ``super-efficient'' simple entropy-accumulation procedures, such as\r\n\r\nR <-  rot_{alpha, n}(R)  XOR  X\r\n\r\nwhere rot_{alpha,n} rotates an n-bit state R by some fixed number alpha. For example, Microsoft's RNG uses alpha=5 for n=32 and alpha=19 for n=64. Where do these numbers come from? Are they good choices? Should rotation be replaced by a better permutation pi of the input bits?\r\n\r\nIn this work we initiate a rigorous study of these pragmatic questions, by modeling the sequence of successive entropic inputs X_1,X_2, ... as independent (but otherwise adversarial) samples from some natural distribution family D. We show a simple but surprisingly powerful connection between entropy accumulation and understanding the Fourier spectrum of distributions in D. Our contribution is as follows.\r\n\r\n- We define 2-monotone distributions as a rich family D that includes relevant real-world distributions (Gaussian, exponential, etc.), but avoids trivial impossibility results.\r\n  \r\n- For any alpha with gcd(alpha,n)=1, we show that rotation accumulates Omega(n) bits of entropy from n independent samples X_1,...,X_n from any (unknown) 2-monotone distribution with entropy k > 1.\r\n\r\n- However, we also show some choices of alpha perform much better than others for a given n. E.g., we show alpha=19 is one of the best choices for n=64; in contrast, alpha=5 is good, but generally worse than alpha=7, for n=32.\r\n\r\n- More generally, given a permutation pi and k > 1, we define a simple parameter, the covering number C_{pi,k}, and show that it characterizes the number of steps before the rule \r\n\r\n(R_1,...,R_n) <- (R_{pi(1)},..., R_{pi(n)}) XOR X \r\n\r\naccumulates nearly n bits of entropy from independent, 2-monotone samples of min-entropy k each.\r\n\r\n- We build a simple permutation pi^*, which achieves nearly optimal C_{pi^*,k} \\approx n/k for all values of k simultaneously, and experimentally validate that it compares favorably with all rotations rot_{alpha,n}.",
                  "authors": [
                    "Yevgeniy Dodis",
                    "Siyao Guo",
                    "Noah Stephens-Davidowitz",
                    "Zhiye Xie"
                  ],
                  "affiliations": "NYU; NYU Shanghai; Cornell University",
                  "category": "Uncategorized",
                  "id": "talk-39"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "20:00",
          "endtime": "20:30",
          "sessions": [
            {
              "id": "session-165",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "20:30",
          "endtime": "21:20",
          "sessions": [
            {
              "id": "session-166",
              "session_title": "Protocols",
              "talks": [
                {
                  "paperId": "155",
                  "title": "A Logarithmic Lower Bound for Oblivious RAM (for all parameters)",
                  "abstract": "An Oblivious RAM (ORAM), introduced by Goldreich and Ostrovsky (J. ACM 1996), is a (probabilistic) RAM that hides its access pattern, i.e., for every input the observed locations accessed are similarly distributed. In recent years there has been great progress both in terms of upper bounds as well as in terms of lower bounds, essentially  pinning down the smallest overhead possible in various settings of parameters. \r\n\r\nWe observe that there is a very natural setting of parameters in which \\emph{no} non-trivial lower bound is known, even not ones in restricted models of computation (like the so called balls and bins model). Let $N$ and $w$ be the number of cells and bit-size of cells, respectively, in the RAM that we wish to simulate obliviously. Denote by $b$ the cell bit-size of the ORAM. \\emph{All} previous ORAM lower bounds have a multiplicative $w/b$ factor which makes them trivial in many settings of parameters of interest.\r\n\r\nIn this work, we prove a new ORAM lower bound that captures this setting (and in all other settings it is at least as good as previous ones, quantitatively). We show that any ORAM must make (amortized)\r\n$$\r\n\\Omega\\left(\\log \\left(\\frac{Nw}{m}\\right)/\\log\\left(\\frac{b}{w}\\right)\\right)\r\n$$\r\nmemory probes for every logical operation. Here, $m$ denotes the bit-size of the local storage of the ORAM. Our lower bound implies that logarithmic overhead in accesses is necessary, even if $ b \\gg w$. Our lower bound is tight for \\emph{all} settings of parameters, up to the $\\log(b/w)$ factor. Our bound also extends to the non-colluding multi-server setting.\r\n\r\nAs an application, we derive the first (unconditional) separation between the overhead needed for ORAMs in the \\emph{online} vs.\\ \\emph{offline} models. Specifically, we show that when $w=\\log N$ and $b,m \\in poly\\log N$, there exists an offline ORAM that makes (on average) $o(1)$ memory probes per logical operation while every online one must make $\\Omega(\\log N/\\log\\log N)$ memory probes per logical operation. No such previous separation was known for any setting of parameters, not even in the balls and bins model.",
                  "authors": [
                    "Ilan Komargodski",
                    "Wei-Kai Lin"
                  ],
                  "affiliations": "Hebrew University and NTT Research; Cornell University",
                  "category": "Uncategorized",
                  "id": "talk-66"
                },
                {
                  "paperId": "170",
                  "title": "Oblivious RAM with Worst-Case Logarithmic Overhead",
                  "abstract": "We present the first Oblivious RAM (ORAM) construction that for $N$ memory blocks supports accesses with \\emph{worst-case} $O(\\log N)$ overhead for any block size $\\Omega(\\log N)$ while requiring a client memory of only a constant number of memory blocks. We rely on the existence of one-way functions and guarantee computational security. Alternatively, we obtain statistical security in the (private) random oracle model. Our result closes a long line of research on fundamental feasibility results for ORAM constructions as logarithmic overhead is necessary.\r\n\r\nThe previous best logarithmic overhead construction only guarantees it in an \\emph{amortized} sense, i.e., logarithmic overhead is achieved only for long enough access sequences, where some of the individual accesses incur $\\Theta(N)$ overhead. The previously best ORAM in terms of \\emph{worst-case} overhead  achieves $O(\\log^2 N/\\log\\log N)$ overhead. \r\n\r\nTechnically, we design a novel de-amortization framework for modern ORAM constructions that use the ``shuffled inputs'' assumption. Our framework significantly departs from all previous de-amortization frameworks, originating from Ostrovsky and Shoup (STOC~'97), that seem to be fundamentally too weak to be applied on modern ORAM constructions.",
                  "authors": [
                    "Gilad Asharov",
                    "Ilan Komargodski",
                    "Wei-Kai Lin",
                    "Elaine Shi"
                  ],
                  "affiliations": "Bar-Ilan University; Hebrew University and NTT Research; Cornell University; Carnegie Mellon University",
                  "category": "Uncategorized",
                  "id": "talk-72"
                },
                {
                  "paperId": "111",
                  "title": "Puncturable Pseudorandom Sets and Private Information Retrieval with Near-Optimal Online Bandwidth and Time",
                  "abstract": "Imagine that one or more non-colluding servers each holds a large public database, e.g., the repository of DNS entries.  Clients would like to access entries in this database without disclosing their queries to the servers.  Classical private information retrieval (PIR) schemes achieve polylogarithmic bandwidth per query, but require the server to perform linear computation per query, which is a deal-breaker in practice.\r\n\r\nSeveral recent works showed, however, that by introducing a one-time, per-client, off-line preprocessing phase, an \\emph{unbounded} number of client queries can be subsequently served\r\nwith sublinear online computation time per query (and the cost of the preprocessing can be amortized over the unboundedly many queries).  Unfortunately, existing preprocessing PIRs\r\nmake undesirable tradeoffs to achieve sublinear online computation: they are either significantly non-optimal in online time or bandwidth, or require the servers to store a linear amount state per client (or even per query), or require polylogarithmically many non-colluding servers.\r\n\r\nWe propose a novel 2-server preprocessing PIR scheme that achieves $\\widetilde{O}(\\sqrt{n})$ online computation per query, while preserving the polylogarithmic online bandwidth of classical PIR\r\nschemes.  Both the online bandwidth and computation are optimal up to a poly-logarithmic factor.\r\nIn our construction, each server stores only the original database and nothing extra, and each online query is served within a single round trip.  Our construction relies on the standard LWE\r\nassumption.  As an important stepping stone, we propose new, more generalized definitions for a cryptographic object called a Privately Puncturable Pseudorandom Set, and give novel constructions that depart significantly from prior approaches.",
                  "authors": [
                    "Elaine Shi",
                    "Waqar Aqeel",
                    "Balakrishnan Chandrasekaran",
                    "Bruce Maggs"
                  ],
                  "affiliations": "CMU; Duke University; Vrije Universiteit Amsterdam; Duke University and Emerald Innovations",
                  "category": "Uncategorized",
                  "id": "talk-55"
                },
                {
                  "paperId": "398",
                  "title": "Authenticated Key Exchange and Signatures with Tight Security in the Standard Model",
                  "abstract": "We construct the first authenticated key exchange protocols that achieve tight security in the standard model. Previous works either relied on techniques that seem to inherently require a random oracle, or achieved only “Multi-Bit-Guess” security, which is not known to compose tightly, for instance, to build a secure channel.\r\nOur constructions are generic, based on digital signatures and key encapsulation mechanisms (KEMs). The main technical challenges we resolve is to determine suitable KEM security notions which on the one hand are strong enough to yield tight security, but at the same time weak enough to be efficiently instantiable in the standard model, based on standard techniques such as universal hash proof systems.\r\nDigital signature schemes with tight multi-user security in presence of adaptive corruptions are a central building block, which is used in all known constructions of tightly-secure AKE with full forward security. We identify a subtle gap in the security proof of the only previously known efficient standard model scheme by Bader et al. (TCC 2015). We develop a new variant, which yields the currently most efficient signature scheme that achieves this strong security notion without random oracles and based on standard hardness assumptions.",
                  "authors": [
                    "Shuai Han",
                    "Tibor Jager",
                    "Eike Kiltz",
                    "Shengli Liu",
                    "Jiaxin Pan",
                    "Doreen Riepel",
                    "Sven Schäge"
                  ],
                  "affiliations": "Shanghai Jiao Tong University; Bergische Universität Wuppertal; Ruhr-Universität Bochum; NTNU, Norway",
                  "category": "Uncategorized",
                  "id": "talk-124"
                },
                {
                  "paperId": "102",
                  "title": "KHAPE: Asymmetric PAKE from Key-Hiding Key Exchange",
                  "abstract": "OPAQUE [Jarecki et al., Eurocrypt 2018] is an asymmetric password authenticated key exchange (aPAKE) protocol that is being developed as an Internet standard and for use within TLS 1.3.  OPAQUE combines an Oblivious PRF (OPRF) with an authenticated key exchange to provide strong security properties, including security against pre-computation attacks (called saPAKE security). However, the security of OPAQUE relies crucially on the integrity of the OPRF. If the latter breaks (by cryptanalysis, quantum attacks or security compromise), the user's password is immediately exposed to an offline dictionary attack. To address this weakness, we present KHAPE, a variant of OPAQUE that does not require the use of an OPRF to achieve aPAKE security, resulting in improved resilience  and performance. An OPRF can be optionally added to KHAPE, for enhanced saPAKE security, but without opening the password to an offline dictionary attack upon OPRF compromise.\r\n\r\nIn addition to resilience to OPRF compromise, a DH-based implementation of KHAPE (using HMQV) offers the best performance among aPAKE protocols in terms of exponentiations with less than the cost of an exponentiation on top of an unauthenticated Diffie-Hellman exchange. KHAPE uses three messages with explicit client authentication and four with explicit server authentication (one more than OPAQUE in the latter case).\r\n\r\nAll results in the paper are proven within the UC framework in the ideal cipher model. Of independent interest is our treatment of \"key-hiding AKE\" which KHAPE uses as a main component, and our UC proofs of AKE security for protocols 3DH (a basis of Signal) and HMQV that we use as efficient instantiations of KHAPE.",
                  "authors": [
                    "Stanislaw Jarecki",
                    "Hugo Krawczyk",
                    "Yanqi Gu"
                  ],
                  "affiliations": "UC Irvine; Algorand Foundation",
                  "category": "Uncategorized",
                  "id": "talk-52"
                }
              ],
              "moderator": "Chair: TBD"
            }
          ]
        },
        {
          "starttime": "21:20",
          "endtime": "21:30",
          "sessions": [
            {
              "id": "session-167",
              "session_title": "Wrap up "
            }
          ]
        }
      ]
    }
  ],
  "database_id": "434"
}
